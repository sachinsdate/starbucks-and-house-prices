{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import us\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Starbucks store count files into Pandas Dataframes\n",
    "\n",
    "df_sb_2017 = pd.read_csv('data\\\\Starbucks_Stores_Feb2017_CORRECT_ZIPS.csv', encoding = \"ISO-8859-1\")\n",
    "df_sb_2022 = pd.read_csv('data\\\\Starbucks_Stores_Jan2022_CORRECT_ZIPS.csv', encoding = \"ISO-8859-1\")\n",
    "df_sb_2024 = pd.read_csv('data\\\\Starbucks_Stores_Sep2024.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Filter to include only US locations (Country == 'US')\n",
    "df_sb_2017_us = df_sb_2017[df_sb_2017['Country'] == 'US']\n",
    "df_sb_2017_us = df_sb_2017_us.reset_index()\n",
    "\n",
    "df_sb_2022_us = df_sb_2022[df_sb_2022['countryCode'] == 'US']\n",
    "df_sb_2022_us = df_sb_2022_us.reset_index()\n",
    "\n",
    "df_sb_2024_us = df_sb_2024[df_sb_2024['country_code'] == 'US']\n",
    "df_sb_2024_us = df_sb_2024_us.reset_index()\n",
    "\n",
    "# Function to clean the Postcode, remove 4-digit extension, \n",
    "# Add a '0' fix to 4-digit and 8-digit postcodes\n",
    "def clean_postcode(postcode):\n",
    "    str_postcode = str(postcode).strip()\n",
    "    if len(str_postcode) == 4:\n",
    "        str_postcode = '0' + str_postcode\n",
    "    if len(str_postcode) == 8:\n",
    "        if str_postcode.startswith('0'):\n",
    "            str_postcode = str_postcode + '0'\n",
    "        else:\n",
    "            str_postcode = '0' + str_postcode\n",
    "    if len(str_postcode) == 9:\n",
    "        return str_postcode[:5]\n",
    "    return str_postcode\n",
    "\n",
    "df_sb_2017_us['Postcode'] = df_sb_2017_us['Postcode'].apply(clean_postcode)\n",
    "df_sb_2022_us['Postcode'] = df_sb_2022_us['postalCode'].apply(clean_postcode)\n",
    "df_sb_2024_us['Postcode'] = df_sb_2024_us['zip_code'].apply(clean_postcode)\n",
    "\n",
    "print('# Starbucks US locations in Feb 2017 file=',len(df_sb_2017_us))\n",
    "print('# Starbucks US locations in Jan 2022 file=',len(df_sb_2022_us))\n",
    "print('# Starbucks US locations in Sep 2024 file=',len(df_sb_2024_us))\n",
    "\n",
    "print('# Unique US ZIP codes with Starbucks in Feb 2017 file=',len(df_sb_2017_us[\"Postcode\"].unique()))\n",
    "print('# Unique US ZIP codes with Starbucks in Jan 2022 file=',len(df_sb_2022_us[\"Postcode\"].unique()))\n",
    "print('# Unique US ZIP codes with Starbucks in Sep 2024 file=',len(df_sb_2024_us[\"Postcode\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get zips in 2022 that were not there in 2017\n",
    "\n",
    "zip_2017 = set(df_sb_2017_us['Postcode'].dropna().unique())\n",
    "\n",
    "zip_2022 = set(df_sb_2022_us['Postcode'].dropna().unique())\n",
    "\n",
    "new_zip_codes_2022 = zip_2022 - zip_2017\n",
    "\n",
    "# Get the list of stores corresponding to these zips.\n",
    "# df_new_zip_new_stores_2022 contains only those ZIPs which did not contain any Starbucks stores in Feb 2017, \n",
    "# and in which one *or more* stores opened at some point between Feb 2017 and Jan 2022\n",
    "df_new_zip_new_stores_2022 = df_sb_2022_us[\n",
    "    df_sb_2022_us['Postcode'].isin(new_zip_codes_2022)\n",
    "].copy()\n",
    "\n",
    "print('Num new stores in 2022 after filtering out ZIP codes already having one store in 2017:',len(df_new_zip_new_stores_2022))\n",
    "\n",
    "# Verify no duplicate store numbers in df_sb_2017_us\n",
    "print('Duplicate store numbers in df_sb_2017_us ?',(df_sb_2017_us.groupby('Store Number').count().sort_values(by='Country', ascending=False).iloc[0]['Country'] > 1))\n",
    "\n",
    "# Verify no duplicate store numbers in df_sb_2022_us\n",
    "print('Duplicate store numbers in df_sb_2022_us ?',(df_sb_2022_us.groupby('storeNumber').count().sort_values(by='countryCode', ascending=False).iloc[0]['countryCode'] > 1))\n",
    "\n",
    "# Verify no duplicate store numbers in df_new_zip_new_stores_2022\n",
    "print('Duplicate store numbers in df_new_zip_new_stores_2022 ?',(df_new_zip_new_stores_2022.groupby('storeNumber').count().sort_values(by='countryCode', ascending=False).iloc[0]['countryCode'] > 1))\n",
    "\n",
    "df_sb_2017_us[df_sb_2017_us['Store Number'].isin(list(df_new_zip_new_stores_2022['storeNumber']))].to_csv('data\\\\common_store_nums.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the store opening date to the stores in df_new_zip_new_stores_2022\n",
    "# \n",
    "# Add an opening_date column to df_new_zip_new_stores_2022 to store the store-opening date. \n",
    "# To populate it, we need to find that store in the starbucks_store_opening_dates_us_ca_14Feb2025.csv file which contain store opening dates,\n",
    "# then copy over the corresponding store opening date for that row into df_new_zip_new_stores_2022.\n",
    "# \n",
    "# starbucks_store_opening_dates_us_ca_14Feb2025 contains Opened, Name, City, Market columns where Market is roughly in the State\\City format.\n",
    "# The df_new_zip_new_stores_2022 does not contain the store name column.\n",
    "# But it does contain the slug column (unique for each store) which can be approximated using the data in the Name, City and State columns in starbucks_store_opening_dates_us_ca_14Feb2025.csv\n",
    "# So we that's the primary technique we use to match rows from starbucks_store_opening_dates_us_ca_14Feb2025 against those in df_new_zip_new_stores_2022\n",
    "# We read starbucks_store_opening_dates_us_ca_14Feb2025.csv into df_sb_store_open_dates,\n",
    "# and populate 2 new columns: state_abbr, and slug using the date in the Name and Market columns.\n",
    "# Then we perform soft-matches of slug in df_sb_store_open_dates with slug in df_new_zip_new_stores_2022,\n",
    "# improving on the accuracy of the match by each time searching only within the corresponding City and State combination.\n",
    "# We also handle some edge cases specially where the slug in df_new_zip_new_stores_2022 may not be in the same sequence of tokens as in the slug in starbucks_store_opening_dates_us_ca_14Feb2025.csv\n",
    "\n",
    "#Read the store open dates file\n",
    " \n",
    "file_path = \"data\\\\starbucks_store_opening_dates_us_ca_14Feb2025.csv\"\n",
    "df_sb_store_open_dates = pd.read_csv(file_path, sep=\"\\t\", parse_dates=['Opened'], dayfirst=False)\n",
    "\n",
    "print('Number of records in the df_sb_store_open_dates=',len(df_sb_store_open_dates))\n",
    "\n",
    "# Preprocess store open dates file.\n",
    "\n",
    "# Add empty 'opening_date' and 'closing_date' columns\n",
    "df_new_zip_new_stores_2022['opening_date'] = pd.NaT\n",
    "df_new_zip_new_stores_2022['closing_date'] = pd.NaT\n",
    "\n",
    "# Function to convert store name to URL slug\n",
    "def generate_slug(name):\n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    # Remove non-alphanumeric characters except spaces and hyphens\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)  \n",
    "    # Replace multiple spaces or hyphens with a single hyphen\n",
    "    name = re.sub(r'[\\s-]+', '-', name).strip('-')  \n",
    "    return name\n",
    "\n",
    "# Populate slug column\n",
    "df_sb_store_open_dates['slug'] = df_sb_store_open_dates['Name'].apply(generate_slug)\n",
    "\n",
    "# List of all U.S. state names (without spaces, as they appear in \"Market\")\n",
    "us_state_names = {state.name.replace(\" \", \"\"): state.abbr for state in us.states.STATES}\n",
    "\n",
    "# Function to extract state abbreviation from 'Market' (format: \"State Name\\AreaName\")\n",
    "def get_state_abbreviation(market):\n",
    "    # Extracts the 2-letter state abbreviation from the 'Market' column.\n",
    "    # Handles cases where state names are concatenated without spaces.\n",
    "    # Extract the state portion before \"\\\"\n",
    "    state_part = market.split('\\\\')[0].strip().lower()\n",
    "\n",
    "    # Find a matching state name in our predefined dictionary\n",
    "    for state_name, state_abbr in us_state_names.items():\n",
    "        if state_part.startswith(state_name.lower()):  # Check if the state name matches\n",
    "            return state_abbr\n",
    "    \n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "# Populate state abbreviation column\n",
    "df_sb_store_open_dates['state_abbr'] = df_sb_store_open_dates['Market'].apply(get_state_abbreviation)\n",
    "\n",
    "# Merge the Opened column from df_sb_store_open_dates into df_new_zip_new_stores_2022\n",
    "# based on slug (prefix match), city, and state\n",
    "\n",
    "updated_count = 0\n",
    "\n",
    "for index, row in df_sb_store_open_dates.iterrows():\n",
    "    slug = row['slug']\n",
    "    city = row['City'].strip().lower()\n",
    "    state = row['state_abbr']\n",
    "\n",
    "    # Find matching stores in df_unique_zip_new_stores_2022\n",
    "    match_index = df_new_zip_new_stores_2022[\n",
    "        (df_new_zip_new_stores_2022['city'].str.strip().str.lower() == city) &\n",
    "        (df_new_zip_new_stores_2022['countrySubdivisionCode'] == state) & \n",
    "        (df_new_zip_new_stores_2022['slug'].str.startswith(slug, na=False))\n",
    "    ].index\n",
    "    updated_count += len(match_index)\n",
    "    if len(match_index) > 1:\n",
    "        print(match_index)\n",
    "    # Assign the opening date\n",
    "    df_new_zip_new_stores_2022.loc[match_index, 'opening_date'] = row['Opened']\n",
    "\n",
    "# Ensure 'opening_date' is in datetime format\n",
    "df_new_zip_new_stores_2022.loc[:, 'opening_date'] = pd.to_datetime(df_new_zip_new_stores_2022['opening_date'])\n",
    "\n",
    "print('Num rows in df_unique_zip_new_stores_2022=',len(df_new_zip_new_stores_2022))\n",
    "print('Num rows updated=',updated_count)\n",
    "\n",
    "# Run another round of slug matching based on matching a sequence of two or more tokens separated by\n",
    "# a '-' in the slug column of df_sb_store_open_dates match the corresponding sequence of \n",
    "# two or more tokens separated by a '-' in the slug column of df_unique_zip_new_stores_2022 \n",
    "def get_token_sequences(slug, min_length=2):\n",
    "    # Extracts all sequences of 'min_length' or more consecutive tokens from a slug.\n",
    "    tokens = slug.split('-')\n",
    "    sequences = []\n",
    "    \n",
    "    # Generate all possible consecutive sequences of at least 'min_length' tokens\n",
    "    for length in range(min_length, len(tokens) + 1):  # Length of sequence\n",
    "        for i in range(len(tokens) - length + 1):  # Start index\n",
    "            sequences.append('-'.join(tokens[i:i+length]))  # Join tokens into a sequence\n",
    "            \n",
    "    return sequences\n",
    "\n",
    "# Define the set of allowed storeNumbers\n",
    "allowed_store_numbers = {\n",
    "    \"16500-171804\", \"63614-297937\", \"66101-299318\", \"65666-298969\", \"62566-296535\",\n",
    "    \"64980-297374\", \"63554-297033\", \"65318-297706\", \"63532-297536\", \"62594-293500\",\n",
    "    \"62906-295007\", \"62036-295964\", \"61739-295506\", \"55077-288371\", \"58902-292643\",\n",
    "    \"57941-291373\", \"56720-289936\", \"55079-288372\", \"48856-261871\", \"53412-284007\",\n",
    "    \"52364-280528\", \"49606-270104\", \"54127-284925\", \"48613-264047\", \"24569-237077\"\n",
    "}\n",
    "\n",
    "# Track how many additional stores get updated\n",
    "additional_updates = 0\n",
    "\n",
    "# Pre-filter df_unique_zip_new_stores_2022 to only include allowed storeNumbers\n",
    "filtered_df = df_new_zip_new_stores_2022[df_new_zip_new_stores_2022['storeNumber'].isin(allowed_store_numbers)]\n",
    "\n",
    "# Iterate over store opening records for a second round of matching\n",
    "for index, row in df_sb_store_open_dates.iterrows():\n",
    "    slug_sequences = get_token_sequences(row['slug'])  # Generate possible token sequences\n",
    "    city = row['City'].strip().lower()\n",
    "    state = row['state_abbr']\n",
    "\n",
    "    # Search for any of these sequences in df_unique_zip_new_stores_2022\n",
    "    for seq in slug_sequences:\n",
    "        match_df = filtered_df[\n",
    "            (filtered_df['opening_date'].isna()) &\n",
    "            (filtered_df['city'].str.strip().str.lower() == city) &\n",
    "            (filtered_df['countrySubdivisionCode'] == state) & \n",
    "            filtered_df['slug'].str.contains(seq, na=False)            \n",
    "        ]\n",
    "        \n",
    "        # If matches found, update opening_date\n",
    "        if not match_df.empty:\n",
    "            df_new_zip_new_stores_2022.loc[match_df.index, 'opening_date'] = row['Opened']\n",
    "            additional_updates += len(match_df)\n",
    "\n",
    "            # Print matched slugs\n",
    "            for match_index in match_df.index:\n",
    "                matched_slug = df_new_zip_new_stores_2022.at[match_index, 'slug']\n",
    "                slug = df_new_zip_new_stores_2022.at[match_index, 'slug']\n",
    "                store_num = df_new_zip_new_stores_2022.at[match_index, 'storeNumber']\n",
    "                store_addr = df_new_zip_new_stores_2022.at[match_index, 'streetAddressLine1']\n",
    "                print(row['slug'],'|',matched_slug,'|',store_addr,'|',store_num)\n",
    "\n",
    "            break  # Stop after the first successful sequence match\n",
    "\n",
    "# Ensure 'opening_date' is in datetime format\n",
    "df_new_zip_new_stores_2022.loc[:, 'opening_date'] = pd.to_datetime(df_new_zip_new_stores_2022['opening_date'])\n",
    "\n",
    "# Display count of additional updates\n",
    "print(f\"Additional stores updated with an opening date in second matching round: {additional_updates}\")\n",
    "\n",
    "print('# Stores with valid open date=',len(df_new_zip_new_stores_2022[df_new_zip_new_stores_2022['opening_date'].notna()]))\n",
    "print('# ZIPs with valid open date=',len(df_new_zip_new_stores_2022[df_new_zip_new_stores_2022['opening_date'].notna()]['Postcode'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each unique zip in df_new_zip_new_stores_2022, identify the year \n",
    "# in which the first Starbucks store opened in that zip\n",
    "# Store this data in a new Dataframe called df_zip_first_open_year\n",
    "\n",
    "# Remember that df_new_zip_new_stores_2022 contains only those ZIPs which did not contain any Starbucks stores in Feb 2017, \n",
    "# and in which one *or more* stores opened at some point between Feb 2017 and Jan 2022\n",
    "\n",
    "df_new_zip_new_stores_2022_copy = df_new_zip_new_stores_2022.dropna(subset=['opening_date']).copy()\n",
    "\n",
    "# Ensure opening_date is in datetime format\n",
    "df_new_zip_new_stores_2022_copy.loc[:, 'opening_date'] = pd.to_datetime(df_new_zip_new_stores_2022_copy['opening_date'])\n",
    "\n",
    "# Add the month and year fields\n",
    "df_new_zip_new_stores_2022_copy['open_year'] = df_new_zip_new_stores_2022_copy['opening_date'].dt.year\n",
    "\n",
    "# Filter rows where first_open_year is between 2017 and 2021.\n",
    "# Doing so filters out the 6 rows where first_open_year < 2017 i.e. the ones with mismatched open dates.\n",
    "# It filters out the 1 row from 2024 as it is outside the time-scope of this study\n",
    "# It also filters the single row from Jan 2022 (there are no other rows with year=2022).\n",
    "# In effect, df_new_zip_new_stores_2022_copy contains stores (and corresponding ZIPs) that opened between Feb 2017 and Dec 2021\n",
    "df_new_zip_new_stores_2022_copy = df_new_zip_new_stores_2022_copy[\n",
    "    (df_new_zip_new_stores_2022_copy['open_year'] >= 2017) & \n",
    "    (df_new_zip_new_stores_2022_copy['open_year'] <= 2021)]\n",
    "\n",
    "# Group by ZIP code and get the first open year and month\n",
    "# Thus, df_zip_first_open_year will contain exactly one row corresponding to each unique ZIP in df_new_zip_new_stores_2022_copy\n",
    "# and that row will contain the year in which the first Starbucks opened in that ZIP at some point between Feb 2017 and Dec 2021\n",
    "df_zip_first_open_year = (\n",
    "    df_new_zip_new_stores_2022_copy\n",
    "    .groupby('Postcode')['open_year']\n",
    "    .min()  # Get the earliest year for each ZIP code\n",
    "    .reset_index()  # Convert back to DataFrame\n",
    ")\n",
    "\n",
    "# Rename the open_year column\n",
    "df_zip_first_open_year = df_zip_first_open_year.rename(columns={'open_year': 'first_open_year'}, errors='raise')\n",
    "\n",
    "print('# zips in df_zip_first_open_year=',len(df_zip_first_open_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FHFA ZIP-5 HPI dataset from https://www.fhfa.gov/data/hpi/datasets?tab=additional-data\n",
    "# NOTE: At present (2024/25), the FHFA dataset goes to only 2023, so the upper end of our study timeframe is \n",
    "# hard-limited to this number for now.\n",
    "# In future (after a few more years), it would be instructive to redownload the FHFA file and redo the study \n",
    "# with the upper bound adjusted to something like 2027. At that point, it would also be useful to consider the data\n",
    "# in the Sep 2024 Starbucks locations file. Till then, the 2024 Starbucks file acts as as a standby, ready to be used once the FHFA\n",
    "# dataset moves beyond 2027. \n",
    "\n",
    "filename = 'data\\\\hpi_at_bdl_zip5_22Oct2024.csv'\n",
    "df_hpi_by_zip_and_year = pd.read_csv(\n",
    "    filename,\n",
    "    delimiter=\",\",\n",
    "    dtype={\"Five-Digit ZIP Code\": str, \"Year\": int}\n",
    ")\n",
    "\n",
    "print('# of rows in FHFA ZIP-5 HPI dataset=',len(df_hpi_by_zip_and_year))\n",
    "\n",
    "# Ensure ZIP is string for proper matching\n",
    "df_hpi_by_zip_and_year['Five-Digit ZIP Code'] = df_hpi_by_zip_and_year['Five-Digit ZIP Code'].astype(str)\n",
    "\n",
    "# Filter only relevant columns\n",
    "df_hpi_filtered = df_hpi_by_zip_and_year[['Five-Digit ZIP Code', 'Year', 'HPI']].copy()\n",
    "\n",
    "# Sort data by ZIP and Year for proper calculations\n",
    "df_hpi_filtered = df_hpi_filtered.sort_values(by=['Five-Digit ZIP Code', 'Year'])\n",
    "\n",
    "# Compute percentage change in HPI per ZIP code\n",
    "df_hpi_filtered['hpi_change'] = df_hpi_filtered.groupby('Five-Digit ZIP Code')['HPI'].pct_change(fill_method=None) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustrative pourposes only, collect the pool of likely control ZIP candidates. These are ZIPs that did not have a Starbucks\n",
    "# store from 2017 through 2024. To compute this set, we pool the ZIPs from the 2017, 2022, and 2024 files.\n",
    "# This union set contains the set of ZIPs which harbored one or more Starbucks stores at least at some time\n",
    "# between Feb 2017 and Oct 2024. Subtracting this set from the unique set of all ZIPs from the FHFA datafile\n",
    "# yields the set of ZIPs that did not have any Starbucks at any time between the Feb 2017 to Oct 2024 timeframe.\n",
    "\n",
    "zip_feb2017 = set(df_sb_2017_us['Postcode'].dropna().unique())\n",
    "zip_jan2022 = set(df_sb_2022_us['Postcode'].dropna().unique())\n",
    "zip_Oct2024 = set(df_sb_2024_us['Postcode'].dropna().unique())\n",
    "\n",
    "starbucks_zips = zip_feb2017.union(zip_jan2022).union(zip_Oct2024)\n",
    "\n",
    "all_zips = set(df_hpi_filtered['Five-Digit ZIP Code'].dropna().unique())\n",
    "\n",
    "starbucks_free_zips_2017_2024 = all_zips - starbucks_zips\n",
    "\n",
    "# Note: We don't use this df anywhere. Instead, we merge df_zip_first_open_year into df_hpi_filtered (the FHFA data) on ZIP code\n",
    "# so that the rows that don't have  df_zip_first_open_year assigned automatically form a part of the control pool. \n",
    "df_starbucks_free_zips_2017_2021 = df_hpi_filtered[\n",
    "    df_hpi_filtered['Five-Digit ZIP Code'].isin(starbucks_free_zips_2017_2024)]\n",
    "\n",
    "print('Number of Starbucks-free zip codes (2017-2024)=',len(starbucks_free_zips_2017_2024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure first_open_year exists in df_zip_first_open_year before merging\n",
    "# df_zip_first_open_year = df_zip_first_open_year[['Postcode', 'first_open_year']]\n",
    "\n",
    "# Merge df_hpi_filtered with df_zip_first_open_year based on ZIP code\n",
    "df_all = df_hpi_filtered.merge(df_zip_first_open_year, \n",
    "                               left_on='Five-Digit ZIP Code', \n",
    "                               right_on='Postcode', \n",
    "                               how='left')\n",
    "\n",
    "# Create event_time as the difference between Year and first_open_year\n",
    "df_all['event_time'] = df_all['Year'] - df_all['first_open_year']\n",
    "\n",
    "# If a ZIP had a Starbucks opening, mark starbucks_opened as 1; otherwise, 0\n",
    "df_all['starbucks_opened'] = df_all['first_open_year'].notna().astype(int)\n",
    "\n",
    "# Drop redundant 'Five-Digit ZIP Code' column after merging\n",
    "df_all.drop(columns=['Postcode'], inplace=True)\n",
    "\n",
    "# Rename ZIP code column for consistency\n",
    "df_all.rename(columns={'Five-Digit ZIP Code': 'Postcode'}, inplace=True)\n",
    "\n",
    "# Reorder columns for readability\n",
    "df_all = df_all[['Postcode', 'Year', 'HPI', 'starbucks_opened', 'first_open_year', 'event_time']]\n",
    "\n",
    "print(len(df_zip_first_open_year), len(df_hpi_filtered), len(df_all))\n",
    "\n",
    "print('# df_all=',len(df_all))\n",
    "print('# starbucks-free rows=',len(df_all[df_all['starbucks_opened']==0]))\n",
    "print('# starbucks-free zips=',len(df_all[df_all['starbucks_opened']==0]['Postcode'].unique()))\n",
    "print('# starbucks rows=',len(df_all[df_all['starbucks_opened']==1]))\n",
    "print('# starbucks zips=',len(df_all[df_all['starbucks_opened']==1]['Postcode'].unique()))\n",
    "print('# first_open_years=',len(df_all[df_all['first_open_year'].notna()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assign pseudo first open years to all starbucks-free zips in the same proportion as they \n",
    "# occur in the treatment zips lying in df_all.\n",
    "# The pseudo first open year enables the TWFE model to compare treatment ZIPs with control ZIPs with the\n",
    "# same first open year, thereby enabling ceteris paribu (other things being equal) style comparisons.  \n",
    "\n",
    "# Get the Dataframe of starbucks free zips\n",
    "df_starbucks_free = df_all[df_all['starbucks_opened']==0].copy()\n",
    "\n",
    "# Group the starbucks free zips by Postcode in their natural order\n",
    "grouped_free = df_starbucks_free.groupby('Postcode', sort=False)\n",
    "\n",
    "# Contains counts of each starbucks-free ZIP in df_all\n",
    "starbucks_free_zip_counts = np.array(grouped_free.size())\n",
    "\n",
    "# Get the actual starbucks free ZIP codes in that same order as the counts array\n",
    "starbucks_free_zips = grouped_free.size().index.values\n",
    "\n",
    "# Get the distribution of first_open years\n",
    "starbucks_first_open_years = np.array(\n",
    "    df_all[df_all['starbucks_opened']==1].groupby('Postcode')['first_open_year'].first())\n",
    "\n",
    "# Choose as many random first open years as the number of Starbucks free zips, \n",
    "# by sampling the distribution in starbucks_first_open_years\n",
    "np.random.seed(42)\n",
    "pseudo_event_years = np.random.choice(a=starbucks_first_open_years, size=len(starbucks_free_zips), replace=True)\n",
    "\n",
    "# Inflate elements in starbucks_first_open_years as many times as the corresponding count \n",
    "# in starbucks_free_zip_counts\n",
    "starbucks_free_zip_pseudo_years = np.repeat(pseudo_event_years, starbucks_free_zip_counts)\n",
    "\n",
    "df_all.loc[df_all['starbucks_opened'] == 0, 'first_open_year'] = starbucks_free_zip_pseudo_years\n",
    "\n",
    "# Recalculate event_time for controls\n",
    "df_all['event_time'] = df_all['Year'] - df_all['first_open_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by ZIP and Year\n",
    "df_all = df_all.sort_values(by=['Postcode', 'Year'])\n",
    "\n",
    "df_all['prev_5yr_avg_hpi_change'] = (\n",
    "    df_all.groupby('Postcode')['HPI']\n",
    "    .pct_change(periods=5, fill_method=None)\n",
    "    .groupby(df_all['Postcode'])  # Ensure rolling is applied within each group\n",
    "    .rolling(window=5, min_periods=1)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index so it aligns with df_all\n",
    ")\n",
    "df_all['prev_4yr_avg_hpi_change'] = (\n",
    "    df_all.groupby('Postcode')['HPI']\n",
    "    .pct_change(periods=4, fill_method=None)\n",
    "    .groupby(df_all['Postcode'])  # Ensure rolling is applied within each group\n",
    "    .rolling(window=4, min_periods=1)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index so it aligns with df_all\n",
    ")\n",
    "df_all['prev_3yr_avg_hpi_change'] = (\n",
    "    df_all.groupby('Postcode')['HPI']\n",
    "    .pct_change(periods=3, fill_method=None)\n",
    "    .groupby(df_all['Postcode'])  # Ensure rolling is applied within each group\n",
    "    .rolling(window=3, min_periods=1)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index so it aligns with df_all\n",
    ")\n",
    "df_all['prev_2yr_avg_hpi_change'] = (\n",
    "    df_all.groupby('Postcode')['HPI']\n",
    "    .pct_change(periods=2, fill_method=None)\n",
    "    .groupby(df_all['Postcode'])  # Ensure rolling is applied within each group\n",
    "    .rolling(window=2, min_periods=1)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index so it aligns with df_all\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a placebo ZIPs group with the same size as df_zip_first_open_year\n",
    "#  (same as real treatment group)\n",
    "\n",
    "# Get a copy of the starbucks-free ZIPs\n",
    "df_placebo_pool = df_all[df_all['starbucks_opened']==0].copy()\n",
    "\n",
    "num_treatment_zips = len(df_all[df_all['starbucks_opened']==1]['Postcode'].unique())\n",
    "placebo_pool_zips = df_all[df_all['starbucks_opened']==0]['Postcode'].unique()\n",
    "\n",
    "np.random.seed(42)\n",
    "num_treatment_zips = len(df_all[df_all['starbucks_opened']==1]['Postcode'].unique())\n",
    "\n",
    "placebo_treatment_zips = np.random.choice(placebo_pool_zips, size=num_treatment_zips, replace=False)\n",
    "\n",
    "df_placebo_pool.loc[df_placebo_pool['Postcode'].isin(placebo_treatment_zips), 'starbucks_opened'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the AGI data into a Dataframe\n",
    "\n",
    "file_path = 'data\\\\'\n",
    "\n",
    "# List to store processed data from each file\n",
    "df_list = []\n",
    "\n",
    "for year in range(12, 22+1):\n",
    "    file_name = file_path + str(year) + 'zpallagi.csv'\n",
    "    print('Processing ' + file_name)\n",
    " \n",
    "    # Read the CSV\n",
    "    df_agi = pd.read_csv(filepath_or_buffer=file_name, header=0)\n",
    "\n",
    "    # Remove invalid ZIP codes (0 and 99999)\n",
    "    df_agi = df_agi[(df_agi['zipcode'] > 0) & (df_agi['zipcode'] <= 99999)]\n",
    "\n",
    "    # Convert ZIP codes to string (to match df_balanced)\n",
    "    df_agi['Postcode'] = df_agi['zipcode'].astype(str).str.zfill(5)\n",
    "\n",
    "    # Aggregate AGI and population for each ZIP and year\n",
    "    df_agi_summary = df_agi.groupby(['Postcode']).agg(\n",
    "        Total_AGI=('A00100', 'sum'),   # Total AGI\n",
    "        Total_Rets=('N1', 'sum') # Total number of returns (proxy for population)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Add Year column\n",
    "    df_agi_summary['Year'] = 2000 + year\n",
    "\n",
    "    # Append to list\n",
    "    df_list.append(df_agi_summary)\n",
    "\n",
    "# Combine all years into a single dataframe\n",
    "df_agi_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Ensure AGI is in actual dollars (convert from thousands)\n",
    "df_agi_all['Total_AGI'] *= 1000\n",
    "\n",
    "#Calculate the average AGI\n",
    "df_agi_all['PC_AGI'] = np.round(df_agi_all['Total_AGI']/df_agi_all['Total_Rets'],2)\n",
    "\n",
    "# Display the final structure\n",
    "df_agi_all.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_placebo_pool with df_agi_all based on ZIP code and year\n",
    "df_all_w_agi = df_placebo_pool.merge(df_agi_all, \n",
    "                               left_on=['Postcode', 'Year'], \n",
    "                               right_on=['Postcode', 'Year'], \n",
    "                               how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all ZIPs, get the rows for the year prior to the one in which Starbucks opened\n",
    "df_all_w_agi_prior_year_row = df_all_w_agi[df_all_w_agi['Year'].eq(df_all_w_agi['first_open_year']-1)].copy()\n",
    "\n",
    "matching_features = ['PC_AGI', 'prev_2yr_avg_hpi_change', 'prev_5yr_avg_hpi_change']\n",
    "\n",
    "df_all_w_agi_prior_year_row = df_all_w_agi_prior_year_row.dropna(subset=matching_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test correlations between the economic measures\n",
    "\n",
    "pd.plotting.scatter_matrix(frame=df_all_w_agi_prior_year_row[matching_features], alpha=0.7, figsize=(10,10), diagonal='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for score based matching (Uses MAHALANOBIS distance)\n",
    "matching_features = ['PC_AGI', 'prev_2yr_avg_hpi_change', 'prev_5yr_avg_hpi_change']\n",
    "\n",
    "# Drop missing values (in case of edge cases)\n",
    "df_matching = df_all_w_agi.dropna(subset=matching_features)\n",
    "\n",
    "# Fit a standard scaler to the matching_features vectors so that they can be standardized by\n",
    "# removing the mean and scaling to unit variance.\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_matching[matching_features].values)\n",
    "\n",
    "# Extract relevant treatment ZIP data\n",
    "df_treatment = df_matching[df_matching['starbucks_opened'] == 1].copy()\n",
    "\n",
    "# Create a dataframe to store matched controls\n",
    "df_matched_control = pd.DataFrame()\n",
    "\n",
    "# Extract unique treatment ZIPs\n",
    "treatment_zips = df_treatment['Postcode'].unique()\n",
    "\n",
    "# Create a pool of candidate control ZIPs\n",
    "df_control_pool = df_matching[df_matching['starbucks_opened'] == 0].copy()\n",
    "\n",
    "count = 1\n",
    "# Loop over each treatment ZIP and find the best match\n",
    "for zip_code in treatment_zips:\n",
    "    # Get first_open_year of treatment ZIP\n",
    "    first_open_year = df_treatment[df_treatment['Postcode'] == zip_code]['first_open_year'].iloc[0]\n",
    "    match_year = first_open_year - 1  # Year before Starbucks opened\n",
    "\n",
    "    # Extract treatment ZIP profile for that match_year\n",
    "    treatment_profile = df_treatment[\n",
    "        (df_treatment['Postcode'] == zip_code) & (df_treatment['Year'] == match_year)\n",
    "        ][matching_features].values\n",
    "    if len(treatment_profile) == 0:\n",
    "        print(count,'/',len(treatment_zips),' ZIP=',zip_code,\n",
    "            ' first_open_year=',first_open_year, ' treatment_profile=',treatment_profile,\n",
    "            ' SKIPPING')\n",
    "        continue\n",
    "\n",
    "    standardized_treatment_profile = scaler.transform(treatment_profile)\n",
    "\n",
    "    # Compute distance between treatment ZIP and all control ZIPs\n",
    "\n",
    "    # Extract control ZIP profiles\n",
    "    control_profiles = df_control_pool[matching_features].values\n",
    "\n",
    "    standardized_control_profiles = scaler.transform(control_profiles)\n",
    "\n",
    "    cov_matrix = np.cov(standardized_control_profiles.T)\n",
    "\n",
    "    # Compute the inverse of the covariance matrix\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "    distances = cdist(standardized_treatment_profile, standardized_control_profiles,\n",
    "        metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "    # print(count,'/',len(treatment_zips),' ZIP=',zip_code,\n",
    "    #     ' first_open_year=',first_open_year, ' treatment_profile=',treatment_profile,\n",
    "    #     ' len(control_profiles)=',len(control_profiles),\n",
    "    #     ' len(distances)=',len(distances[0]))\n",
    "\n",
    "    # Find the best matching control ZIP\n",
    "    best_match_idx = np.argmin(distances)\n",
    "    best_match_zip = df_control_pool.iloc[best_match_idx]\n",
    "\n",
    "    df_best_match_zip_rows = df_control_pool[df_control_pool['Postcode']==best_match_zip['Postcode']]\n",
    "\n",
    "    # Append the best match to df_matched_control\n",
    "    df_matched_control = pd.concat([df_matched_control, df_best_match_zip_rows])\n",
    "\n",
    "    # Sort matched control ZIPs for consistency\n",
    "    df_matched_control = df_matched_control.sort_values(by=['Postcode', 'Year'])\n",
    "\n",
    "    # Remove the selected control ZIP from further consideration\n",
    "    df_control_pool = df_control_pool[df_control_pool['Postcode'] != best_match_zip['Postcode']]\n",
    "\n",
    "    count = count + 1\n",
    "\n",
    "# Concatenate treatment and matched control ZIPs\n",
    "df_balanced = pd.concat([df_treatment, df_matched_control], ignore_index=True)\n",
    "\n",
    "df_balanced = df_balanced.astype({'Postcode': 'string', 'Year': 'int32', 'HPI': 'float64', 'starbucks_opened': 'int32', 'first_open_year': 'int32',\n",
    "       'event_time': 'int32', 'prev_5yr_avg_hpi_change': 'float64', 'prev_4yr_avg_hpi_change': 'float64',\n",
    "       'prev_3yr_avg_hpi_change': 'float64', 'prev_2yr_avg_hpi_change': 'float64'})\n",
    "\n",
    "print('# df_treatment=',len(df_treatment))\n",
    "print('# Unique ZIPs in df_treatment=',len(df_treatment['Postcode'].unique()))\n",
    "print('# df_control_pool=',len(df_control_pool))\n",
    "print('# Unique ZIPs in df_control_pool=',len(df_control_pool['Postcode'].unique()))\n",
    "print('# df_matched_control=',len(df_matched_control))\n",
    "print('# Unique ZIPs in df_matched_control=',len(df_matched_control['Postcode'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only event window (e.g., -5 to +5 years around event)\n",
    "df_balanced_5 = df_balanced[(df_balanced['event_time'] >= -5) & (df_balanced['event_time'] <= 5)]\n",
    "\n",
    "# Create event time dummies correctly\n",
    "event_time_dummies = pd.get_dummies(df_balanced_5['event_time'], drop_first=True, prefix='t').astype(int)\n",
    "\n",
    "# Rename columns: Replace negative signs and ensure valid variable names\n",
    "event_time_dummies.columns = [col.replace('-', 'm_').replace('t_', 't') for col in event_time_dummies.columns]\n",
    "\n",
    "# Merge with the main dataset\n",
    "df_balanced_5 = pd.concat([df_balanced_5, event_time_dummies], axis=1)\n",
    "\n",
    "df_balanced_5['Postcode'] = df_balanced_5['Postcode'].astype('object')\n",
    "\n",
    "print(\"Num years=\",df_balanced_5['Year'].unique())\n",
    "\n",
    "print(\"# treated ZIPs=\",len(df_balanced_5[df_balanced_5['starbucks_opened']==0]['Postcode'].unique()), \", # matched control ZIPs=\",len(df_balanced_5[df_balanced_5['starbucks_opened']==1]['Postcode'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the regression model on the PLACEBO dataset\n",
    "\n",
    "# With pre_trend_slope\n",
    "formula = \"np.log(HPI) ~ \" + \" + \".join(event_time_dummies.columns) + \" + PC_AGI + prev_2yr_avg_hpi_change + prev_5yr_avg_hpi_change + C(Year)\"\n",
    "\n",
    "# Run the regression\n",
    "model = ols(formula, data=df_balanced_5)\n",
    "\n",
    "cluster_groups = df_balanced_5.loc[model.data.row_labels, 'Postcode']\n",
    "\n",
    "model_results = model.fit(cov_type='cluster', cov_kwds={'groups': cluster_groups})\n",
    "\n",
    "#model_results = model.fit(cov_type='HC1')\n",
    "\n",
    "# Display results\n",
    "print(model_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a joint F-test on the pre=entry coefficients for tm_4, tm_3, tm_2, and tm_1.\n",
    "# This is one way to look at the parallel trends assumption in a TWFE setting. \n",
    "hypotheses = 'tm_4 = tm_3 = tm_2 = tm_1 = 0'\n",
    "f_test = model_results.f_test(hypotheses)\n",
    "print(f_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model results to file\n",
    "\n",
    "# Extracts relevant statistics and coefficients from model results and saves them to a JSON file.\n",
    "def save_model_results_to_json(model_results, filename):\n",
    "    \n",
    "    # Extract model summary statistics\n",
    "    summary_data = {\n",
    "        \"Dep. Variable\": model_results.model.endog_names,\n",
    "        \"Model\": \"OLS\",  # Explicitly stating the model type\n",
    "        \"Method\": \"Least Squares\",\n",
    "        \"Date\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"Time\": datetime.datetime.now().strftime(\"%H:%M:%S\"),\n",
    "        \"N\": int(model_results.nobs),\n",
    "        \"Df Residuals\": int(model_results.df_resid),\n",
    "        \"Df Model\": int(model_results.df_model),\n",
    "        \"Covariance Type\": model_results.cov_type,\n",
    "        \"R-squared\": model_results.rsquared,\n",
    "        \"Adj. R-squared\": model_results.rsquared_adj,\n",
    "        \"F-statistic\": model_results.fvalue if model_results.fvalue is not None else None,\n",
    "        \"Prob (F-statistic)\": model_results.f_pvalue if model_results.f_pvalue is not None else None,\n",
    "        \"Log-Likelihood\": model_results.llf,\n",
    "        \"AIC\": model_results.aic,\n",
    "        \"BIC\": model_results.bic,\n",
    "        \"Coefficients\": {}\n",
    "    }\n",
    "\n",
    "    # Extract coefficients, standard errors, and confidence intervals\n",
    "    coef_data = model_results.params\n",
    "    std_err_data = model_results.bse\n",
    "    conf_int_data = model_results.conf_int()\n",
    "\n",
    "    for param in coef_data.index:\n",
    "        summary_data[\"Coefficients\"][param] = {\n",
    "            \"value\": coef_data[param],\n",
    "            \"std_err\": std_err_data[param],\n",
    "            \"conf_int_low\": conf_int_data.loc[param, 0],\n",
    "            \"conf_int_high\": conf_int_data.loc[param, 1]\n",
    "        }\n",
    "\n",
    "    # Write to JSON file\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json.dump(summary_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"Model results saved to {filename}\")\n",
    "\n",
    "\n",
    "save_model_results_to_json(model_results, \"data\\\\placebo_study_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a comparison of the coefficients from the real and placebo studies\n",
    "\n",
    "# Reads JSON file and extracts coefficients and confidence intervals for event-time dummies.\n",
    "def load_coefficients(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract coefficients for event-time dummies only (tm_4 to t_5)\n",
    "    event_times = [\"tm_4\", \"tm_3\", \"tm_2\", \"tm_1\", \"t0\", \"t1\", \"t2\", \"t3\", \"t4\", \"t5\"]\n",
    "    coefs = [data[\"Coefficients\"][t][\"value\"] for t in event_times]\n",
    "    conf_low = [data[\"Coefficients\"][t][\"conf_int_low\"] for t in event_times]\n",
    "    conf_high = [data[\"Coefficients\"][t][\"conf_int_high\"] for t in event_times]\n",
    "\n",
    "    return np.array(event_times), np.array(coefs), np.array(conf_low), np.array(conf_high)\n",
    "\n",
    "# Plots event-study coefficients for real and placebo tests.\n",
    "def plot_event_study(real_study_file, placebo_file):\n",
    "    \n",
    "    # Load real study coefficients\n",
    "    event_times, coefs_real, conf_low_real, conf_high_real = load_coefficients(real_study_file)\n",
    "    \n",
    "    # Load placebo study coefficients\n",
    "    _, coefs_placebo, conf_low_placebo, conf_high_placebo = load_coefficients(placebo_file)\n",
    "\n",
    "    # Convert event times for plotting (x-axis)\n",
    "    event_numbers = np.arange(len(event_times))\n",
    "\n",
    "    # Plot Real Study\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    plt.plot(event_numbers, coefs_real, marker='o', linestyle='-', color='royalblue', label=\"Real Study\")\n",
    "    plt.fill_between(event_numbers, conf_low_real, conf_high_real, color='royalblue', alpha=0.2)\n",
    "\n",
    "    # Plot Placebo Study\n",
    "    plt.plot(event_numbers, coefs_placebo, marker='s', linestyle='--', color='r', label=\"Placebo Study\")\n",
    "    plt.fill_between(event_numbers, conf_low_placebo, conf_high_placebo, color='r', alpha=0.2)\n",
    "\n",
    "    # Mark the Starbucks Entry Event\n",
    "    plt.axvline(4, color='k', linestyle='--', linewidth=1)\n",
    "    plt.text(4, max(conf_high_real) * 0.9, \"Starbucks Entry\", \n",
    "            horizontalalignment='center', verticalalignment='bottom', fontsize=10, color='black', fontweight='bold')\n",
    "\n",
    "    # Formatting\n",
    "    plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    plt.xticks(event_numbers, list(range(-4, 6)))\n",
    "    plt.xlabel(\"Event Time\")\n",
    "    plt.ylabel(\"Coefficient Estimate\")\n",
    "    #plt.title(\"Event Study Coefficients: Real vs. Placebo\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "real_study_results_file = \"data\\\\real_study_results.json\"\n",
    "placebo_study_results_file = \"data\\\\placebo_study_results.json\"\n",
    "\n",
    "plot_event_study(real_study_results_file, placebo_study_results_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
