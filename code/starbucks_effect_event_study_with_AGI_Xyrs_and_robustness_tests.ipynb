{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import us\n",
    "import patsy\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.linalg import pinv, lstsq\n",
    "from scipy.stats import norm, chi2\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Starbucks store count files into Pandas Dataframes\n",
    "\n",
    "df_sb_2017 = pd.read_csv('data\\\\Starbucks_Stores_Feb2017_CORRECT_ZIPS.csv', encoding = \"ISO-8859-1\")\n",
    "df_sb_2022 = pd.read_csv('data\\\\Starbucks_Stores_Jan2022_CORRECT_ZIPS.csv', encoding = \"ISO-8859-1\")\n",
    "df_sb_2024 = pd.read_csv('data\\\\Starbucks_Stores_Sep2024.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Filter to include only US locations (Country == 'US')\n",
    "df_sb_2017_us = df_sb_2017[df_sb_2017['Country'] == 'US']\n",
    "df_sb_2017_us = df_sb_2017_us.reset_index()\n",
    "\n",
    "df_sb_2022_us = df_sb_2022[df_sb_2022['countryCode'] == 'US']\n",
    "df_sb_2022_us = df_sb_2022_us.reset_index()\n",
    "\n",
    "df_sb_2024_us = df_sb_2024[df_sb_2024['country_code'] == 'US']\n",
    "df_sb_2024_us = df_sb_2024_us.reset_index()\n",
    "\n",
    "# Function to clean the Postcode, remove 4-digit extension, \n",
    "# Add a '0' fix to 4-digit and 8-digit postcodes\n",
    "def clean_postcode(postcode):\n",
    "    str_postcode = str(postcode).strip()\n",
    "    if len(str_postcode) == 4:\n",
    "        str_postcode = '0' + str_postcode\n",
    "    if len(str_postcode) == 8:\n",
    "        if str_postcode.startswith('0'):\n",
    "            str_postcode = str_postcode + '0'\n",
    "        else:\n",
    "            str_postcode = '0' + str_postcode\n",
    "    if len(str_postcode) == 9:\n",
    "        return str_postcode[:5]\n",
    "    return str_postcode\n",
    "\n",
    "df_sb_2017_us['Postcode'] = df_sb_2017_us['Postcode'].apply(clean_postcode)\n",
    "df_sb_2022_us['Postcode'] = df_sb_2022_us['postalCode'].apply(clean_postcode)\n",
    "df_sb_2024_us['Postcode'] = df_sb_2024_us['zip_code'].apply(clean_postcode)\n",
    "\n",
    "print('# Starbucks US locations in Feb 2017 file=',len(df_sb_2017_us))\n",
    "print('# Starbucks US locations in Jan 2022 file=',len(df_sb_2022_us))\n",
    "print('# Starbucks US locations in Sep 2024 file=',len(df_sb_2024_us))\n",
    "\n",
    "print('# Unique US ZIP codes with Starbucks in Feb 2017 file=',len(df_sb_2017_us[\"Postcode\"].unique()))\n",
    "print('# Unique US ZIP codes with Starbucks in Jan 2022 file=',len(df_sb_2022_us[\"Postcode\"].unique()))\n",
    "print('# Unique US ZIP codes with Starbucks in Sep 2024 file=',len(df_sb_2024_us[\"Postcode\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get zips in 2022 that were not there in 2017\n",
    "\n",
    "zip_2017 = set(df_sb_2017_us['Postcode'].dropna().unique())\n",
    "\n",
    "zip_2022 = set(df_sb_2022_us['Postcode'].dropna().unique())\n",
    "\n",
    "new_zip_codes_2022 = zip_2022 - zip_2017\n",
    "\n",
    "# Get the list of stores corresponding to these zips.\n",
    "# df_new_zip_new_stores_2022 contains only those ZIPs which did not contain any Starbucks stores in Feb 2017, \n",
    "# and in which one *or more* stores opened at some point between Feb 2017 and Jan 2022\n",
    "df_new_zip_new_stores_2022 = df_sb_2022_us[\n",
    "    df_sb_2022_us['Postcode'].isin(new_zip_codes_2022)\n",
    "].copy()\n",
    "\n",
    "print('Num new stores in 2022 after filtering out ZIP codes already having one store in 2017:',len(df_new_zip_new_stores_2022))\n",
    "\n",
    "# Verify no duplicate store numbers in df_sb_2017_us\n",
    "print('Duplicate store numbers in df_sb_2017_us ?',(df_sb_2017_us.groupby('Store Number').count().sort_values(by='Country', ascending=False).iloc[0]['Country'] > 1))\n",
    "\n",
    "# Verify no duplicate store numbers in df_sb_2022_us\n",
    "print('Duplicate store numbers in df_sb_2022_us ?',(df_sb_2022_us.groupby('storeNumber').count().sort_values(by='countryCode', ascending=False).iloc[0]['countryCode'] > 1))\n",
    "\n",
    "# Verify no duplicate store numbers in df_new_zip_new_stores_2022\n",
    "print('Duplicate store numbers in df_new_zip_new_stores_2022 ?',(df_new_zip_new_stores_2022.groupby('storeNumber').count().sort_values(by='countryCode', ascending=False).iloc[0]['countryCode'] > 1))\n",
    "\n",
    "df_sb_2017_us[df_sb_2017_us['Store Number'].isin(list(df_new_zip_new_stores_2022['storeNumber']))].to_csv('dataData\\\\common_store_nums.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the store opening date to the stores in df_new_zip_new_stores_2022\n",
    "# \n",
    "# Add an opening_date column to df_new_zip_new_stores_2022 to store the store-opening date. \n",
    "# To populate it, we need to find that store in the starbucks_store_opening_dates_us_ca_14Feb2025.csv file which contain store opening dates,\n",
    "# then copy over the corresponding store opening date for that row into df_new_zip_new_stores_2022.\n",
    "# \n",
    "# starbucks_store_opening_dates_us_ca_14Feb2025 contains Opened, Name, City, Market columns where Market is roughly in the State\\City format.\n",
    "# The df_new_zip_new_stores_2022 does not contain the store name column.\n",
    "# But it does contain the slug column (unique for each store) which can be approximated using the data in the Name, City and State columns in starbucks_store_opening_dates_us_ca_14Feb2025.csv\n",
    "# So we that's the primary technique we use to match rows from starbucks_store_opening_dates_us_ca_14Feb2025 against those in df_new_zip_new_stores_2022\n",
    "# We read starbucks_store_opening_dates_us_ca_14Feb2025.csv into df_sb_store_open_dates,\n",
    "# and populate 2 new columns: state_abbr, and slug using the date in the Name and Market columns.\n",
    "# Then we perform soft-matches of slug in df_sb_store_open_dates with slug in df_new_zip_new_stores_2022,\n",
    "# improving on the accuracy of the match by each time searching only within the corresponding City and State combination.\n",
    "# We also handle some edge cases specially where the slug in df_new_zip_new_stores_2022 may not be in the same sequence of tokens as in the slug in starbucks_store_opening_dates_us_ca_14Feb2025.csv\n",
    "\n",
    "#Read the store open dates file\n",
    " \n",
    "file_path = \"data\\\\starbucks_store_opening_dates_us_ca_14Feb2025.csv\"\n",
    "df_sb_store_open_dates = pd.read_csv(file_path, sep=\"\\t\", parse_dates=['Opened'], dayfirst=False)\n",
    "\n",
    "print('Number of records in the df_sb_store_open_dates=',len(df_sb_store_open_dates))\n",
    "\n",
    "# Preprocess store open dates file.\n",
    "\n",
    "# Add empty 'opening_date' and 'closing_date' columns\n",
    "df_new_zip_new_stores_2022['opening_date'] = pd.NaT\n",
    "df_new_zip_new_stores_2022['closing_date'] = pd.NaT\n",
    "\n",
    "# Function to convert store name to URL slug\n",
    "def generate_slug(name):\n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    # Remove non-alphanumeric characters except spaces and hyphens\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)  \n",
    "    # Replace multiple spaces or hyphens with a single hyphen\n",
    "    name = re.sub(r'[\\s-]+', '-', name).strip('-')  \n",
    "    return name\n",
    "\n",
    "# Populate slug column\n",
    "df_sb_store_open_dates['slug'] = df_sb_store_open_dates['Name'].apply(generate_slug)\n",
    "\n",
    "# List of all U.S. state names (without spaces, as they appear in \"Market\")\n",
    "us_state_names = {state.name.replace(\" \", \"\"): state.abbr for state in us.states.STATES}\n",
    "\n",
    "# Extracts the 2-letter state abbreviation from the 'Market' column.\n",
    "# Handles cases where state names are concatenated without spaces.\n",
    "def get_state_abbreviation(market):\n",
    "    # Extract the state portion before \"\\\"\n",
    "    state_part = market.split('\\\\')[0].strip().lower()\n",
    "\n",
    "    # Find a matching state name in our predefined dictionary\n",
    "    for state_name, state_abbr in us_state_names.items():\n",
    "        if state_part.startswith(state_name.lower()):  # Check if the state name matches\n",
    "            return state_abbr\n",
    "    \n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "# Populate state abbreviation column\n",
    "df_sb_store_open_dates['state_abbr'] = df_sb_store_open_dates['Market'].apply(get_state_abbreviation)\n",
    "\n",
    "# Merge the Opened column from df_sb_store_open_dates into df_new_zip_new_stores_2022\n",
    "# based on slug (prefix match), city, and state\n",
    "\n",
    "updated_count = 0\n",
    "\n",
    "for index, row in df_sb_store_open_dates.iterrows():\n",
    "    slug = row['slug']\n",
    "    city = row['City'].strip().lower()\n",
    "    state = row['state_abbr']\n",
    "\n",
    "    # Find matching stores in df_unique_zip_new_stores_2022\n",
    "    match_index = df_new_zip_new_stores_2022[\n",
    "        (df_new_zip_new_stores_2022['city'].str.strip().str.lower() == city) &\n",
    "        (df_new_zip_new_stores_2022['countrySubdivisionCode'] == state) & \n",
    "        (df_new_zip_new_stores_2022['slug'].str.startswith(slug, na=False))\n",
    "    ].index\n",
    "    updated_count += len(match_index)\n",
    "    if len(match_index) > 1:\n",
    "        print(match_index)\n",
    "    # Assign the opening date\n",
    "    df_new_zip_new_stores_2022.loc[match_index, 'opening_date'] = row['Opened']\n",
    "\n",
    "# Ensure 'opening_date' is in datetime format\n",
    "df_new_zip_new_stores_2022.loc[:, 'opening_date'] = pd.to_datetime(df_new_zip_new_stores_2022['opening_date'])\n",
    "\n",
    "print('Num rows in df_unique_zip_new_stores_2022=',len(df_new_zip_new_stores_2022))\n",
    "print('Num rows updated=',updated_count)\n",
    "\n",
    "# Run another round of slug matching based on matching a sequence of two or more tokens separated by\n",
    "# a '-' in the slug column of df_sb_store_open_dates match the corresponding sequence of \n",
    "# two or more tokens separated by a '-' in the slug column of df_unique_zip_new_stores_2022 \n",
    "# Extracts all sequences of 'min_length' or more consecutive tokens from a slug.\n",
    "def get_token_sequences(slug, min_length=2):\n",
    "    tokens = slug.split('-')\n",
    "    sequences = []\n",
    "    \n",
    "    # Generate all possible consecutive sequences of at least 'min_length' tokens\n",
    "    for length in range(min_length, len(tokens) + 1):  # Length of sequence\n",
    "        for i in range(len(tokens) - length + 1):  # Start index\n",
    "            sequences.append('-'.join(tokens[i:i+length]))  # Join tokens into a sequence\n",
    "            \n",
    "    return sequences\n",
    "\n",
    "# Define the set of allowed storeNumbers\n",
    "allowed_store_numbers = {\n",
    "    \"16500-171804\", \"63614-297937\", \"66101-299318\", \"65666-298969\", \"62566-296535\",\n",
    "    \"64980-297374\", \"63554-297033\", \"65318-297706\", \"63532-297536\", \"62594-293500\",\n",
    "    \"62906-295007\", \"62036-295964\", \"61739-295506\", \"55077-288371\", \"58902-292643\",\n",
    "    \"57941-291373\", \"56720-289936\", \"55079-288372\", \"48856-261871\", \"53412-284007\",\n",
    "    \"52364-280528\", \"49606-270104\", \"54127-284925\", \"48613-264047\", \"24569-237077\"\n",
    "}\n",
    "\n",
    "# Track how many additional stores get updated\n",
    "additional_updates = 0\n",
    "\n",
    "# Pre-filter df_unique_zip_new_stores_2022 to only include allowed storeNumbers\n",
    "filtered_df = df_new_zip_new_stores_2022[df_new_zip_new_stores_2022['storeNumber'].isin(allowed_store_numbers)]\n",
    "\n",
    "# Iterate over store opening records for a second round of matching\n",
    "for index, row in df_sb_store_open_dates.iterrows():\n",
    "    slug_sequences = get_token_sequences(row['slug'])  # Generate possible token sequences\n",
    "    city = row['City'].strip().lower()\n",
    "    state = row['state_abbr']\n",
    "\n",
    "    # Search for any of these sequences in df_unique_zip_new_stores_2022\n",
    "    for seq in slug_sequences:\n",
    "        match_df = filtered_df[\n",
    "            (filtered_df['opening_date'].isna()) &\n",
    "            (filtered_df['city'].str.strip().str.lower() == city) &\n",
    "            (filtered_df['countrySubdivisionCode'] == state) & \n",
    "            filtered_df['slug'].str.contains(seq, na=False)            \n",
    "        ]\n",
    "        \n",
    "        # If matches found, update opening_date\n",
    "        if not match_df.empty:\n",
    "            df_new_zip_new_stores_2022.loc[match_df.index, 'opening_date'] = row['Opened']\n",
    "            additional_updates += len(match_df)\n",
    "\n",
    "            # Print matched slugs\n",
    "            for match_index in match_df.index:\n",
    "                matched_slug = df_new_zip_new_stores_2022.at[match_index, 'slug']\n",
    "                slug = df_new_zip_new_stores_2022.at[match_index, 'slug']\n",
    "                store_num = df_new_zip_new_stores_2022.at[match_index, 'storeNumber']\n",
    "                store_addr = df_new_zip_new_stores_2022.at[match_index, 'streetAddressLine1']\n",
    "                print(row['slug'],'|',matched_slug,'|',store_addr,'|',store_num)\n",
    "\n",
    "            break  # Stop after the first successful sequence match\n",
    "\n",
    "# Ensure 'opening_date' is in datetime format\n",
    "df_new_zip_new_stores_2022.loc[:, 'opening_date'] = pd.to_datetime(df_new_zip_new_stores_2022['opening_date'])\n",
    "\n",
    "# Display count of additional updates\n",
    "print(f\"Additional stores updated with an opening date in second matching round: {additional_updates}\")\n",
    "\n",
    "print('# Stores with valid open date=',len(df_new_zip_new_stores_2022[df_new_zip_new_stores_2022['opening_date'].notna()]))\n",
    "print('# ZIPs with valid open date=',len(df_new_zip_new_stores_2022[df_new_zip_new_stores_2022['opening_date'].notna()]['Postcode'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each unique zip in df_new_zip_new_stores_2022, identify the year \n",
    "# in which the first Starbucks store opened in that zip\n",
    "# Store this data in a new Dataframe called df_zip_first_open_year\n",
    "\n",
    "# Remember that df_new_zip_new_stores_2022 contains only those ZIPs which did not contain any Starbucks stores in Feb 2017, \n",
    "# and in which one *or more* stores opened at some point between Feb 2017 and Jan 2022\n",
    "\n",
    "df_new_zip_new_stores_2022_copy = df_new_zip_new_stores_2022.dropna(subset=['opening_date']).copy()\n",
    "\n",
    "# Ensure opening_date is in datetime format\n",
    "df_new_zip_new_stores_2022_copy.loc[:, 'opening_date'] = pd.to_datetime(df_new_zip_new_stores_2022_copy['opening_date'])\n",
    "\n",
    "# Add the month and year fields\n",
    "df_new_zip_new_stores_2022_copy['open_year'] = df_new_zip_new_stores_2022_copy['opening_date'].dt.year\n",
    "\n",
    "# Filter rows where first_open_year is between 2017 and 2021.\n",
    "# Doing so filters out the 6 rows where first_open_year < 2017 i.e. the ones with mismatched open dates.\n",
    "# It filters out the 1 row from 2024 as it is outside the time-scope of this study\n",
    "# It also filters the single row from Jan 2022 (there are no other rows with year=2022).\n",
    "# In effect, df_new_zip_new_stores_2022_copy contains stores (and corresponding ZIPs) that opened between Feb 2017 and Dec 2021\n",
    "df_new_zip_new_stores_2022_copy = df_new_zip_new_stores_2022_copy[\n",
    "    (df_new_zip_new_stores_2022_copy['open_year'] >= 2017) & \n",
    "    (df_new_zip_new_stores_2022_copy['open_year'] <= 2021)]\n",
    "\n",
    "# Group by ZIP code and get the first open year and month\n",
    "# Thus, df_zip_first_open_year will contain exactly one row corresponding to each unique ZIP in df_new_zip_new_stores_2022_copy\n",
    "# and that row will contain the year in which the first Starbucks opened in that ZIP at some point between Feb 2017 and Dec 2021\n",
    "df_zip_first_open_year = (\n",
    "    df_new_zip_new_stores_2022_copy\n",
    "    .groupby('Postcode')['open_year']\n",
    "    .min()  # Get the earliest year for each ZIP code\n",
    "    .reset_index()  # Convert back to DataFrame\n",
    ")\n",
    "\n",
    "# Rename the open_year column\n",
    "df_zip_first_open_year = df_zip_first_open_year.rename(columns={'open_year': 'first_open_year'}, errors='raise')\n",
    "\n",
    "print('# zips in df_zip_first_open_year=',len(df_zip_first_open_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FHFA ZIP-5 HPI dataset from https://www.fhfa.gov/data/hpi/datasets?tab=additional-data\n",
    "# NOTE: At present (2024/25), the FHFA dataset goes to only 2023, so the upper end of our study timeframe is \n",
    "# hard-limited to this number for now.\n",
    "# In future (after a few more years), it would be instructive to redownload the FHFA file and redo the study \n",
    "# with the upper bound adjusted to something like 2027. At that point, it would also be useful to consider the data\n",
    "# in the Sep 2024 Starbucks locations file. Till then, the 2024 Starbucks file acts as as a standby, ready to be used once the FHFA\n",
    "# dataset moves beyond 2027. \n",
    "filename = 'data\\\\hpi_at_bdl_zip5_22Oct2024.csv'\n",
    "df_hpi_by_zip_and_year = pd.read_csv(\n",
    "    filename,\n",
    "    delimiter=\",\",\n",
    "    dtype={\"Five-Digit ZIP Code\": str, \"Year\": int}\n",
    ")\n",
    "\n",
    "print('# of rows in FHFA ZIP-5 HPI dataset=',len(df_hpi_by_zip_and_year))\n",
    "\n",
    "# Ensure ZIP is string for proper matching\n",
    "df_hpi_by_zip_and_year['Five-Digit ZIP Code'] = df_hpi_by_zip_and_year['Five-Digit ZIP Code'].astype(str)\n",
    "\n",
    "# Filter only relevant columns\n",
    "df_hpi_filtered = df_hpi_by_zip_and_year[['Five-Digit ZIP Code', 'Year', 'HPI']].copy()\n",
    "\n",
    "# Sort data by ZIP and Year for proper calculations\n",
    "df_hpi_filtered = df_hpi_filtered.sort_values(by=['Five-Digit ZIP Code', 'Year'])\n",
    "\n",
    "# Compute percentage change in HPI per ZIP code\n",
    "df_hpi_filtered['hpi_change'] = df_hpi_filtered.groupby('Five-Digit ZIP Code')['HPI'].pct_change(fill_method=None) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustrative pourposes only, collect the pool of likely control ZIP candidates. These are ZIPs that did not have a Starbucks\n",
    "# store from 2017 through 2024. To compute this set, we pool the ZIPs from the 2017, 2022, and 2024 files.\n",
    "# This union set contains the set of ZIPs which harbored one or more Starbucks stores at least at some time\n",
    "# between Feb 2017 and Oct 2024. Subtracting this set from the unique set of all ZIPs from the FHFA datafile\n",
    "# yields the set of ZIPs that did not have any Starbucks at any time between the Feb 2017 to Oct 2024 timeframe.\n",
    "\n",
    "zip_feb2017 = set(df_sb_2017_us['Postcode'].dropna().unique())\n",
    "zip_jan2022 = set(df_sb_2022_us['Postcode'].dropna().unique())\n",
    "zip_Oct2024 = set(df_sb_2024_us['Postcode'].dropna().unique())\n",
    "\n",
    "starbucks_zips = zip_feb2017.union(zip_jan2022).union(zip_Oct2024)\n",
    "\n",
    "all_zips = set(df_hpi_filtered['Five-Digit ZIP Code'].dropna().unique())\n",
    "\n",
    "starbucks_free_zips_2017_2024 = all_zips - starbucks_zips\n",
    "\n",
    "# Note: We don't use this df anywhere. Instead, we merge df_zip_first_open_year into df_hpi_filtered (the FHFA data) on ZIP code\n",
    "# so that the rows that don't have  df_zip_first_open_year assigned automatically form a part of the control pool. \n",
    "df_starbucks_free_zips_2017_2021 = df_hpi_filtered[\n",
    "    df_hpi_filtered['Five-Digit ZIP Code'].isin(starbucks_free_zips_2017_2024)]\n",
    "\n",
    "print('Number of Starbucks-free zip codes (2017-2024)=',len(starbucks_free_zips_2017_2024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure first_open_year exists in df_zip_first_open_year before merging\n",
    "# df_zip_first_open_year = df_zip_first_open_year[['Postcode', 'first_open_year']]\n",
    "\n",
    "# Merge df_hpi_filtered with df_zip_first_open_year based on ZIP code\n",
    "df_all = df_hpi_filtered.merge(df_zip_first_open_year, \n",
    "                               left_on='Five-Digit ZIP Code', \n",
    "                               right_on='Postcode', \n",
    "                               how='left')\n",
    "\n",
    "# Create event_time as the difference between Year and first_open_year\n",
    "df_all['event_time'] = df_all['Year'] - df_all['first_open_year']\n",
    "\n",
    "# If a ZIP had a Starbucks opening, mark starbucks_opened as 1; otherwise, 0\n",
    "df_all['starbucks_opened'] = df_all['first_open_year'].notna().astype(int)\n",
    "\n",
    "# Drop redundant 'Five-Digit ZIP Code' column after merging\n",
    "df_all.drop(columns=['Postcode'], inplace=True)\n",
    "\n",
    "# Rename ZIP code column for consistency\n",
    "df_all.rename(columns={'Five-Digit ZIP Code': 'Postcode'}, inplace=True)\n",
    "\n",
    "# Reorder columns for readability\n",
    "df_all = df_all[['Postcode', 'Year', 'HPI', 'starbucks_opened', 'first_open_year', 'event_time']]\n",
    "\n",
    "print(len(df_zip_first_open_year), len(df_hpi_filtered), len(df_all))\n",
    "\n",
    "print('# df_all=',len(df_all))\n",
    "print('# starbucks-free rows=',len(df_all[df_all['starbucks_opened']==0]))\n",
    "print('# starbucks-free zips=',len(df_all[df_all['starbucks_opened']==0]['Postcode'].unique()))\n",
    "print('# starbucks rows=',len(df_all[df_all['starbucks_opened']==1]))\n",
    "print('# starbucks zips=',len(df_all[df_all['starbucks_opened']==1]['Postcode'].unique()))\n",
    "print('# first_open_years=',len(df_all[df_all['first_open_year'].notna()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the HPI profile of ZIPs in which Starbucks opened?\n",
    "\n",
    "# For all treatment ZIPs, get the rows for the year prior to the one in which Starbucks opened\n",
    "df_treatment_prior_year_row = df_all[df_all['starbucks_opened'].eq(1) & \n",
    "    df_all['Year'].eq(df_all['first_open_year']-1)].copy()\n",
    "\n",
    "df_treatment_prior_year_row = df_treatment_prior_year_row.dropna(subset=['HPI'])\n",
    "\n",
    "#Print the descritive stats\n",
    "print(df_treatment_prior_year_row['HPI'].describe())\n",
    "\n",
    "# Draw a histogram of HPI for the year prior to the one in which Starbucks opened\n",
    "\n",
    "# Set the style for the plot\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_treatment_prior_year_row['HPI'], bins=20, color=\"royalblue\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"House Price Index (HPI)\", fontsize=10)\n",
    "plt.ylabel(\"Count of Starbucks Opened\", fontsize=10)\n",
    "plt.title(\"Distribution of HPI across treatment ZIPs in the year prior to year of first-entry\", fontsize=16)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a histogram of store open years for the treatment pool\n",
    "\n",
    "df_treatment_hpi_prior_year_counts = df_treatment_prior_year_row.groupby('first_open_year').count()\n",
    "\n",
    "# Set the style for the plot\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=np.int32(np.array(df_treatment_hpi_prior_year_counts.index)), y=df_treatment_hpi_prior_year_counts['Year'], color=\"royalblue\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"Year of First Entry\", fontsize=10)\n",
    "plt.ylabel(\"Count of Starbucks\", fontsize=10)\n",
    "#plt.title(\"Distribution of Year of First Entry for Treatment ZIPs\", fontsize=16)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load uszips.csv\n",
    "filename = 'data\\\\uszips.csv'\n",
    "\n",
    "df_uszips = pd.read_csv(filename, dtype={\"zip\": str})  # Ensure ZIPs are strings\n",
    "\n",
    "# Select relevant columns\n",
    "df_uszips = df_uszips[['zip', 'city', 'state_id', 'state_name', 'county_name']]\n",
    "\n",
    "# Convert column names for easy merging\n",
    "df_uszips.rename(columns={'zip': 'Postcode'}, inplace=True)\n",
    "\n",
    "# Merge uszips data into the df_treatment_prior_year_row dataframe\n",
    "df_treatment_prior_year_row = df_treatment_prior_year_row.merge(df_uszips, on='Postcode', how='left')\n",
    "\n",
    "# Get the group-wise counts\n",
    "df_treatment_hpi_state_counts = df_treatment_prior_year_row.groupby('state_id').count()\n",
    "\n",
    "# Set the style for the plot\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_treatment_hpi_state_counts, x='state_id', y='Postcode', color=\"royalblue\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"State\", fontsize=10)\n",
    "plt.ylabel(\"Count of Starbucks\", fontsize=10)\n",
    "plt.title(\"Distribution of Treatment ZIPs by State\", fontsize=16)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "print('Number of states covered in treatment group=',len(df_treatment_hpi_state_counts.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assign pseudo first open years to all starbucks-free zips in the same proportion as they \n",
    "# occur in the treatment zips lying in df_all.\n",
    "# The pseudo first open year enables the TWFE model to compare treatment ZIPs with control ZIPs with the\n",
    "# same first open year, thereby enabling ceteris paribu (other things being equal) style comparisons.  \n",
    "\n",
    "# Get the Dataframe of starbucks free zips\n",
    "df_starbucks_free = df_all[df_all['starbucks_opened']==0].copy()\n",
    "\n",
    "# Group the starbucks free zips by Postcode in their natural order\n",
    "grouped_free = df_starbucks_free.groupby('Postcode', sort=False)\n",
    "\n",
    "# Contains counts of each starbucks-free ZIP in df_all\n",
    "starbucks_free_zip_counts = np.array(grouped_free.size())\n",
    "\n",
    "# Get the actual starbucks free ZIP codes in that same order as the counts array\n",
    "starbucks_free_zips = grouped_free.size().index.values\n",
    "\n",
    "# Get the distribution of first_open years\n",
    "starbucks_first_open_years = np.array(\n",
    "    df_all[df_all['starbucks_opened']==1].groupby('Postcode')['first_open_year'].first())\n",
    "\n",
    "# Choose as many random first open years as the number of Starbucks free zips, \n",
    "# by sampling the distribution in starbucks_first_open_years\n",
    "np.random.seed(42)\n",
    "pseudo_event_years = np.random.choice(a=starbucks_first_open_years, size=len(starbucks_free_zips), replace=True)\n",
    "\n",
    "# Inflate elements in starbucks_first_open_years as many times as the corresponding count \n",
    "# in starbucks_free_zip_counts\n",
    "starbucks_free_zip_pseudo_years = np.repeat(pseudo_event_years, starbucks_free_zip_counts)\n",
    "\n",
    "df_all.loc[df_all['starbucks_opened'] == 0, 'first_open_year'] = starbucks_free_zip_pseudo_years\n",
    "\n",
    "# Recalculate event_time for controls\n",
    "df_all['event_time'] = df_all['Year'] - df_all['first_open_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prev_Xyr_avg_hpi_change\n",
    "\n",
    "# Sort data by ZIP and Year\n",
    "df_all = df_all.sort_values(by=['Postcode', 'Year'])\n",
    "\n",
    "df_all['prev_5yr_avg_hpi_change'] = (\n",
    "    df_all.groupby('Postcode')['HPI']\n",
    "    .pct_change(periods=5, fill_method=None)\n",
    "    .groupby(df_all['Postcode'])  # Ensure rolling is applied within each group\n",
    "    .rolling(window=5, min_periods=1)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index so it aligns with df_all\n",
    ")\n",
    "df_all['prev_4yr_avg_hpi_change'] = (\n",
    "    df_all.groupby('Postcode')['HPI']\n",
    "    .pct_change(periods=4, fill_method=None)\n",
    "    .groupby(df_all['Postcode'])  # Ensure rolling is applied within each group\n",
    "    .rolling(window=4, min_periods=1)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index so it aligns with df_all\n",
    ")\n",
    "df_all['prev_3yr_avg_hpi_change'] = (\n",
    "    df_all.groupby('Postcode')['HPI']\n",
    "    .pct_change(periods=3, fill_method=None)\n",
    "    .groupby(df_all['Postcode'])  # Ensure rolling is applied within each group\n",
    "    .rolling(window=3, min_periods=1)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index so it aligns with df_all\n",
    ")\n",
    "df_all['prev_2yr_avg_hpi_change'] = (\n",
    "    df_all.groupby('Postcode')['HPI']\n",
    "    .pct_change(periods=2, fill_method=None)\n",
    "    .groupby(df_all['Postcode'])  # Ensure rolling is applied within each group\n",
    "    .rolling(window=2, min_periods=1)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index so it aligns with df_all\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the AGI data into a Dataframe\n",
    "# Download the ZIP files from the U.S. census website and unzip them.\n",
    "# You should get files named '12zpallagi.csv' to '22zpallagi.csv' \n",
    "# Place the unzipped files in your data directory \n",
    "file_path = 'data\\\\'\n",
    "\n",
    "# List to store processed data from each file\n",
    "df_list = []\n",
    "\n",
    "for year in range(12, 22+1):\n",
    "    file_name = file_path + str(year) + 'zpallagi.csv'\n",
    "    print('Processing ' + file_name)\n",
    " \n",
    "    # Read the CSV\n",
    "    df_agi = pd.read_csv(filepath_or_buffer=file_name, header=0)\n",
    "\n",
    "    # Remove invalid ZIP codes (0 and 99999)\n",
    "    df_agi = df_agi[(df_agi['zipcode'] > 0) & (df_agi['zipcode'] <= 99999)]\n",
    "\n",
    "    # Convert ZIP codes to string (to match df_balanced)\n",
    "    df_agi['Postcode'] = df_agi['zipcode'].astype(str).str.zfill(5)\n",
    "\n",
    "    # Aggregate AGI and population for each ZIP and year\n",
    "    df_agi_summary = df_agi.groupby(['Postcode']).agg(\n",
    "        Total_AGI=('A00100', 'sum'),   # Total AGI\n",
    "        Total_Rets=('N1', 'sum') # Total number of returns (proxy for population)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Add Year column\n",
    "    df_agi_summary['Year'] = 2000 + year\n",
    "\n",
    "    # Append to list\n",
    "    df_list.append(df_agi_summary)\n",
    "\n",
    "# Combine all years into a single dataframe\n",
    "df_agi_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Ensure AGI is in actual dollars (convert from thousands)\n",
    "df_agi_all['Total_AGI'] *= 1000\n",
    "\n",
    "#Calculate the average AGI\n",
    "df_agi_all['PC_AGI'] = np.round(df_agi_all['Total_AGI']/df_agi_all['Total_Rets'],2)\n",
    "\n",
    "# Display the final structure\n",
    "df_agi_all.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_all with df_agi_all based on ZIP code and year\n",
    "df_all_w_agi = df_all.merge(df_agi_all, \n",
    "                               left_on=['Postcode', 'Year'], \n",
    "                               right_on=['Postcode', 'Year'], \n",
    "                               how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the sequence of dataframe joins that have occurred so far to get us to df_all_w_agi:\n",
    "# df_zip_first_open_year (n=524) with df_hpi_filtered (n=648254) to get df_all (n=648254)\n",
    "# df_all (n=648254) with df_agi_all (n=306295) to get df_all_w_agi (n=648254)\n",
    "print(len(df_zip_first_open_year), len(df_hpi_filtered), len(df_all), len(df_agi_all), len(df_all_w_agi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all ZIPs, get the rows for the year prior to the one in which Starbucks opened\n",
    "df_all_w_agi_prior_year_row = df_all_w_agi[df_all_w_agi['Year'].eq(df_all_w_agi['first_open_year']-1)].copy()\n",
    "\n",
    "matching_features = ['PC_AGI', 'prev_2yr_avg_hpi_change', 'prev_5yr_avg_hpi_change']\n",
    "\n",
    "df_all_w_agi_prior_year_row = df_all_w_agi_prior_year_row.dropna(subset=matching_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test correlations between the economic measures\n",
    "\n",
    "\n",
    "# Generate the scatter matrix\n",
    "scatter_matrix_fig = pd.plotting.scatter_matrix(\n",
    "    frame=df_all_w_agi_prior_year_row[matching_features], figsize=(10,10), c='royalblue', alpha=0.5\n",
    ")\n",
    "\n",
    "# Change histogram colors\n",
    "for ax in scatter_matrix_fig.diagonal():\n",
    "    ax.hist(df_all_w_agi_prior_year_row[ax.get_xlabel()], color='royalblue', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for score based matching (Uses MAHALANOBIS distance)\n",
    "matching_features = ['PC_AGI', 'prev_2yr_avg_hpi_change', 'prev_5yr_avg_hpi_change']\n",
    "\n",
    "# Drop all rows where the features to match have missing values\n",
    "# df_matching will be used to draw the pool of treatment and matching control ZIPs\n",
    "df_matching = df_all_w_agi.dropna(subset=matching_features)\n",
    "\n",
    "# Fit a standard scaler to the matching_features vectors so that they can be standardized by\n",
    "# removing the mean and scaling to unit variance.\n",
    "scaler = StandardScaler()\n",
    "# Note that df_matching[matching_features].values is an array of size (len(df_matching), len(matching_features)) i.e. (200597, 3)\n",
    "scaler = scaler.fit(df_matching[matching_features].values)\n",
    "\n",
    "# Extract the rows corresponding to ZIPs in which Starbucks opened in the Feb 2017 to Dec 2021 timeframe.\n",
    "# Recall that this set of rows appear in df_matching as a result of the following sequence of Dataframe joins:\n",
    "# df_zip_first_open_year with df_hpi_filtered to get df_all\n",
    "# df_all with df_agi_all to get df_all_w_agi\n",
    "df_treatment = df_matching[df_matching['starbucks_opened'] == 1].copy()\n",
    "\n",
    "# Create a dataframe to store matched controls\n",
    "df_matched_control = pd.DataFrame()\n",
    "\n",
    "# Extract the set of unique treatment ZIPs from the set of treatment rows \n",
    "treatment_zips = df_treatment['Postcode'].unique()\n",
    "\n",
    "# Create a pool of candidate control ZIPs\n",
    "df_control_pool = df_matching[df_matching['starbucks_opened'] == 0].copy()\n",
    "\n",
    "# Define a function to compute Mahalanobis distance\n",
    "def mahalanobis(u, v, VI):\n",
    "    delta = u - v\n",
    "    return np.sqrt(np.dot(np.dot(delta, VI), delta.T))\n",
    "\n",
    "count = 1\n",
    "# Loop over each treatment ZIP and find the best match\n",
    "for zip_code in treatment_zips:\n",
    "    # Get first_open_year of treatment ZIP\n",
    "    first_open_year = df_treatment[df_treatment['Postcode'] == zip_code]['first_open_year'].iloc[0]\n",
    "    match_year = first_open_year - 1  # Year before Starbucks opened\n",
    "\n",
    "    # Extract treatment ZIP profile for that match_year\n",
    "    treatment_profile = df_treatment[\n",
    "        (df_treatment['Postcode'] == zip_code) & (df_treatment['Year'] == match_year)\n",
    "        ][matching_features].values\n",
    "    if len(treatment_profile) == 0:\n",
    "        print(count,'/',len(treatment_zips),' ZIP=',zip_code,\n",
    "            ' first_open_year=',first_open_year, ' treatment_profile=',treatment_profile,\n",
    "            ' SKIPPING')\n",
    "        continue\n",
    "\n",
    "    standardized_treatment_profile = scaler.transform(treatment_profile)\n",
    "\n",
    "    # Compute distance between treatment ZIP and all control ZIPs\n",
    "\n",
    "    # Extract control ZIP profiles\n",
    "    control_profiles = df_control_pool[matching_features].values\n",
    "\n",
    "    standardized_control_profiles = scaler.transform(control_profiles)\n",
    "\n",
    "    cov_matrix = np.cov(standardized_control_profiles.T)\n",
    "\n",
    "    # Compute the inverse of the covariance matrix\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "    distances = cdist(standardized_treatment_profile, standardized_control_profiles,\n",
    "        metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "    # print(count,'/',len(treatment_zips),' ZIP=',zip_code,\n",
    "    #     ' first_open_year=',first_open_year, ' treatment_profile=',treatment_profile,\n",
    "    #     ' len(control_profiles)=',len(control_profiles),\n",
    "    #     ' len(distances)=',len(distances[0]))\n",
    "\n",
    "    # Find the best matching control ZIP\n",
    "    best_match_idx = np.argmin(distances)\n",
    "    best_match_zip = df_control_pool.iloc[best_match_idx]\n",
    "\n",
    "    df_best_match_zip_rows = df_control_pool[df_control_pool['Postcode']==best_match_zip['Postcode']]\n",
    "\n",
    "    # Append the best match to df_matched_control\n",
    "    df_matched_control = pd.concat([df_matched_control, df_best_match_zip_rows])\n",
    "\n",
    "    # Sort matched control ZIPs for consistency\n",
    "    df_matched_control = df_matched_control.sort_values(by=['Postcode', 'Year'])\n",
    "\n",
    "    # Remove the selected control ZIP from further consideration\n",
    "    df_control_pool = df_control_pool[df_control_pool['Postcode'] != best_match_zip['Postcode']]\n",
    "\n",
    "    count = count + 1\n",
    "\n",
    "# Concatenate treatment and matched control ZIPs\n",
    "df_balanced = pd.concat([df_treatment, df_matched_control], ignore_index=True)\n",
    "\n",
    "df_balanced = df_balanced.astype({'Postcode': 'string', 'Year': 'int32', 'HPI': 'float64', 'starbucks_opened': 'int32', 'first_open_year': 'int32',\n",
    "       'event_time': 'int32', 'prev_5yr_avg_hpi_change': 'float64', 'prev_4yr_avg_hpi_change': 'float64',\n",
    "       'prev_3yr_avg_hpi_change': 'float64', 'prev_2yr_avg_hpi_change': 'float64'})\n",
    "\n",
    "print('# df_treatment=',len(df_treatment))\n",
    "print('# Unique ZIPs in df_treatment=',len(df_treatment['Postcode'].unique()))\n",
    "print('# df_control_pool=',len(df_control_pool))\n",
    "print('# Unique ZIPs in df_control_pool=',len(df_control_pool['Postcode'].unique()))\n",
    "print('# df_matched_control=',len(df_matched_control))\n",
    "print('# Unique ZIPs in df_matched_control=',len(df_matched_control['Postcode'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot of treatment and matched control groups demonstrates the high-quality matching we perform between treatment and control groups.\n",
    "# This, to a large extent, allows us to make the claim that the parallel trends assumption will be obeyed by our data despite.\n",
    "# E[log(HPI)_0_T_post​ − log(HPI)_0_T_pre​] = E[log(HPI)_0_C_post​ − log(HPI)_0_C_pre​]].\n",
    "# Where, T = treatment group, C = Control group, post/pre = post/pre treatment, 0/1 = actually treated (received) / not treated (did not receive) Starbucks  \n",
    "# In words: in the absence of treatment, the treated and control groups would have experienced the same average outcome trend over time.\n",
    "\n",
    "\n",
    "# Combine treatment and matched control data for visualization\n",
    "df_treatment_scaled = pd.DataFrame(\n",
    "    scaler.transform(df_treatment[matching_features]), columns=matching_features)\n",
    "df_treatment_scaled[\"Group\"] = \"Treatment\"\n",
    "\n",
    "df_matched_control_scaled = pd.DataFrame(\n",
    "    scaler.transform(df_matched_control[matching_features]), columns=matching_features)\n",
    "df_matched_control_scaled[\"Group\"] = \"Matched Control\"\n",
    "\n",
    "df_combined = pd.concat([df_treatment_scaled, df_matched_control_scaled])\n",
    "\n",
    "# Convert to long format for seaborn\n",
    "df_long = df_combined.melt(id_vars=[\"Group\"], var_name=\"Economic Indicator\", value_name=\"Standardized Value\")\n",
    "\n",
    "# Plot distributions to compare treatment and matched controls\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(\n",
    "    data=df_long,\n",
    "    x=\"Economic Indicator\",\n",
    "    y=\"Standardized Value\",\n",
    "    hue=\"Group\",\n",
    "    split=True,          # requires exactly two hue levels\n",
    "    inner=\"quartile\",    # show quartiles inside each violin\n",
    "    cut=0,               # (optional) don’t extend KDE beyond data\n",
    "    density_norm=\"width\"        # (optional) keep widths comparable\n",
    ")\n",
    "#plt.title(\"Comparison of Standardized Matching Variables Between Treatment and Matched Controls\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num years= [2016 2017 2018 2019 2020 2021 2022 2015 2014 2013 2012]\n",
      "# treated ZIPs= 505 , # matched control ZIPs= 507\n"
     ]
    }
   ],
   "source": [
    "# Keep only the rows that fit within event window (e.g., -5 to +5 years around event)\n",
    "df_balanced_5 = df_balanced[(df_balanced['event_time'] >= -5) & (df_balanced['event_time'] <= 5)]\n",
    "\n",
    "# Create event time dummies correctly\n",
    "event_time_dummies = pd.get_dummies(df_balanced_5['event_time'], drop_first=True, prefix='t').astype(int)\n",
    "\n",
    "# Rename columns: Replace negative signs and ensure valid variable names\n",
    "event_time_dummies.columns = [col.replace('-', 'm_').replace('t_', 't') for col in event_time_dummies.columns]\n",
    "\n",
    "# Merge with the main dataset\n",
    "df_balanced_5 = pd.concat([df_balanced_5, event_time_dummies], axis=1)\n",
    "\n",
    "df_balanced_5['Postcode'] = df_balanced_5[\"Postcode\"].astype('object')\n",
    "\n",
    "print(\"Num years=\",df_balanced_5['Year'].unique())\n",
    "\n",
    "print(\"# treated ZIPs=\",len(df_balanced_5[df_balanced_5['starbucks_opened']==0]['Postcode'].unique()), \", # matched control ZIPs=\",len(df_balanced_5[df_balanced_5['starbucks_opened']==1]['Postcode'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the regression model without and with the pre_trend_slope \n",
    "\n",
    "formula = \"np.log(HPI) ~ \" + \" + \".join(event_time_dummies.columns) + \" + PC_AGI + prev_2yr_avg_hpi_change + prev_5yr_avg_hpi_change + C(Year)\"\n",
    "\n",
    "model = ols(formula, data=df_balanced_5)\n",
    "\n",
    "cluster_groups = df_balanced_5.loc[model.data.row_labels, 'Postcode']\n",
    "\n",
    "model_results = model.fit(cov_type='cluster', cov_kwds={'groups': cluster_groups})\n",
    "\n",
    "#model_results = model.fit(cov_type='HC1')\n",
    "\n",
    "# Display results\n",
    "print(model_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confirm our judgement that there is no anticipation by performing a progressive donut.\n",
    "# We'll repeatedly test for joint zero-ness of pre-trend coefficients (using a Wald test) and \n",
    "# associated pre-trend slope by progressively removing the pre-trends at $\\tau$ = -1, -2, -3.\n",
    "\n",
    "# Parse tau for names like: tm_4, tm_3, tm_2, tm_1, t0, t1, ... t5\n",
    "def parse_tau(colname: str) -> int:\n",
    "    s = colname.strip().lower()\n",
    "    if s.startswith(\"tm_\"):        # negative taus, e.g. tm_3 -> -3\n",
    "        return -int(s.split(\"tm_\")[1])\n",
    "    if s.startswith(\"t\"):          # nonnegative taus, e.g. t4 -> 4, t0 -> 0\n",
    "        return int(s[1:])\n",
    "    raise ValueError(f\"Unexpected event-time name: {colname}\")\n",
    "\n",
    "def log_to_pct(x):  # for convenience in the table\n",
    "    return 100.0 * (np.exp(x) - 1.0)\n",
    "\n",
    "# Progressive 'anticipation' donuts on TWFE leads.\n",
    "# Returns a DataFrame with mean pre (log & %), Wald χ² stat and p-value.\n",
    "# Assumes event-time columns are named like: tm_4, tm_3, tm_2, tm_1, t0, t1, ..., t5\n",
    "def twfe_pre_donut_tests(model_results, event_time_dummies, drop_sizes=(1,2,3)):\n",
    "    # Order event-time coefficients by τ\n",
    "    tau_cols = list(event_time_dummies.columns)\n",
    "    taus = pd.Index([parse_tau(c) for c in tau_cols], name=\"tau\")\n",
    "    order = np.argsort(taus.values)\n",
    "    taus = taus[order]\n",
    "    tau_cols = [tau_cols[i] for i in order]\n",
    "\n",
    "    # Pull cluster-robust estimates & covariance for the event-time dummies\n",
    "    beta = model_results.params.loc[tau_cols].to_numpy()\n",
    "    Sigma = model_results.cov_params().loc[tau_cols, tau_cols].to_numpy()\n",
    "\n",
    "    pre_all = [t for t in taus if t < 0]  # leads only\n",
    "    if len(pre_all) == 0:\n",
    "        raise ValueError(\"No pre-treatment (lead) coefficients found among tm_*.\")\n",
    "\n",
    "    rows = []\n",
    "    for k in drop_sizes:\n",
    "        drop = set(range(-k, 0))   # {-1}, {-2,-1}, {-3,-2,-1}\n",
    "        keep = [t for t in pre_all if t not in drop]\n",
    "        if not keep:\n",
    "            continue\n",
    "\n",
    "        idx = [list(taus).index(t) for t in keep]\n",
    "        theta = beta[idx]\n",
    "        V = Sigma[np.ix_(idx, idx)]\n",
    "\n",
    "        mean_log = float(theta.mean())\n",
    "        mean_pct = log_to_pct(mean_log)\n",
    "\n",
    "        W = float(theta.T @ pinv(V) @ theta)\n",
    "        p = float(1 - chi2.cdf(W, df=len(idx)))\n",
    "\n",
    "        rows.append({\n",
    "            \"drop_k\": k,\n",
    "            \"kept_taus\": keep,\n",
    "            \"mean_pre_log\": mean_log,\n",
    "            \"mean_pre_%\": mean_pct,\n",
    "            \"Wald_chi2\": W,\n",
    "            \"df\": len(idx),\n",
    "            \"p_value\": p\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "twfe_donuts = twfe_pre_donut_tests(model_results, event_time_dummies, drop_sizes=(1,2,3))\n",
    "print(twfe_donuts.to_string(index=False,\n",
    "      formatters={\"mean_pre_log\": \"{:.4f}\".format,\n",
    "                  \"mean_pre_%\": \"{:.2f}\".format,\n",
    "                  \"Wald_chi2\": \"{:.2f}\".format,\n",
    "                  \"p_value\": \"{:.3g}\".format}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drop_k</th>\n",
       "      <th>kept_taus</th>\n",
       "      <th>mean_pre_log</th>\n",
       "      <th>mean_pre_%</th>\n",
       "      <th>Wald_chi2</th>\n",
       "      <th>df</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[-4, -3, -2]</td>\n",
       "      <td>0.068103</td>\n",
       "      <td>7.047580</td>\n",
       "      <td>9.778713</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[-4, -3]</td>\n",
       "      <td>0.051452</td>\n",
       "      <td>5.279868</td>\n",
       "      <td>7.673235</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[-4]</td>\n",
       "      <td>0.036196</td>\n",
       "      <td>3.685876</td>\n",
       "      <td>6.973952</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   drop_k     kept_taus  mean_pre_log  mean_pre_%  Wald_chi2  df   p_value\n",
       "0       1  [-4, -3, -2]      0.068103    7.047580   9.778713   3  0.020544\n",
       "1       2      [-4, -3]      0.051452    5.279868   7.673235   2  0.021566\n",
       "2       3          [-4]      0.036196    3.685876   6.973952   1  0.008270"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twfe_donuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model results to file\n",
    "\n",
    "# Extracts relevant statistics and coefficients from model results and saves them to a JSON file.\n",
    "def save_model_results_to_json(model_results, filename):\n",
    "    \n",
    "    # Extract model summary statistics\n",
    "    summary_data = {\n",
    "        \"Dep. Variable\": model_results.model.endog_names,\n",
    "        \"Model\": \"OLS\",  # Explicitly stating the model type\n",
    "        \"Method\": \"Least Squares\",\n",
    "        \"Date\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"Time\": datetime.datetime.now().strftime(\"%H:%M:%S\"),\n",
    "        \"N\": int(model_results.nobs),\n",
    "        \"Df Residuals\": int(model_results.df_resid),\n",
    "        \"Df Model\": int(model_results.df_model),\n",
    "        \"Covariance Type\": model_results.cov_type,\n",
    "        \"R-squared\": model_results.rsquared,\n",
    "        \"Adj. R-squared\": model_results.rsquared_adj,\n",
    "        \"F-statistic\": model_results.fvalue if model_results.fvalue is not None else None,\n",
    "        \"Prob (F-statistic)\": model_results.f_pvalue if model_results.f_pvalue is not None else None,\n",
    "        \"Log-Likelihood\": model_results.llf,\n",
    "        \"AIC\": model_results.aic,\n",
    "        \"BIC\": model_results.bic,\n",
    "        \"Coefficients\": {}\n",
    "    }\n",
    "\n",
    "    # Extract coefficients, standard errors, and confidence intervals\n",
    "    coef_data = model_results.params\n",
    "    std_err_data = model_results.bse\n",
    "    conf_int_data = model_results.conf_int()\n",
    "\n",
    "    for param in coef_data.index:\n",
    "        summary_data[\"Coefficients\"][param] = {\n",
    "            \"value\": coef_data[param],\n",
    "            \"std_err\": std_err_data[param],\n",
    "            \"conf_int_low\": conf_int_data.loc[param, 0],\n",
    "            \"conf_int_high\": conf_int_data.loc[param, 1]\n",
    "        }\n",
    "\n",
    "    # Write to JSON file\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json.dump(summary_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"Model results saved to {filename}\")\n",
    "\n",
    "\n",
    "save_model_results_to_json(model_results, \"results\\\\real_study_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Partial-R² analysis\n",
    "\n",
    "# Full model formula\n",
    "full_formula = \"np.log(HPI) ~ \" + \" + \".join(event_time_dummies.columns) + \\\n",
    "               \" + PC_AGI + prev_2yr_avg_hpi_change + prev_5yr_avg_hpi_change + C(Year)\"\n",
    "\n",
    "# Fit the full model\n",
    "full_model_results = smf.ols(full_formula, data=df_balanced_5).fit()\n",
    "full_r2 = full_model_results.rsquared_adj\n",
    "\n",
    "# Variables to test for importance\n",
    "variables_to_test = [\n",
    "    'PC_AGI', 'prev_2yr_avg_hpi_change', 'prev_5yr_avg_hpi_change'\n",
    "] + event_time_dummies.columns.tolist()\n",
    "\n",
    "# Dictionary to store partial R²\n",
    "partial_r2 = {}\n",
    "\n",
    "# Loop through each variable and calculate the drop in R²\n",
    "for var in tqdm(variables_to_test):\n",
    "    print (var)\n",
    "    reduced_vars = [v for v in variables_to_test if v != var]\n",
    "    reduced_formula = \"np.log(HPI) ~ \" + \" + \".join(reduced_vars) + \" + C(Year)\"\n",
    "    print(reduced_formula)\n",
    "    reduced_model_results = smf.ols(reduced_formula, data=df_balanced_5).fit()\n",
    "    reduced_r2 = reduced_model_results.rsquared_adj\n",
    "    partial_r2[var] = full_r2 - reduced_r2\n",
    "\n",
    "print(partial_r2)\n",
    "\n",
    "# Sort by importance\n",
    "sorted_r2 = sorted(partial_r2.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(sorted_r2)\n",
    "\n",
    "# Display\n",
    "df_importances = pd.DataFrame(sorted_r2, columns=['Variable', 'Partial_Adj_R2_Drop'])\n",
    "print(df_importances)\n",
    "\n",
    "# Set the style for the plot\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=['PC_AGI', 'prev_5yr', 'prev_2yr', 'D(τ=5)', 'D(τ=4)', 'D(τ=3)', 'D(τ=2)', 'D(τ=1)', 'D(τ=0)', 'D(τ=-1)', 'D(τ=-2)', 'D(τ=-3)', 'D(τ=-4)'], y=df_importances['Partial_Adj_R2_Drop'], color=\"royalblue\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"Variable\", fontsize=10)\n",
    "plt.ylabel(\"Drop in Adjusted R-squared\", fontsize=10)\n",
    "plt.title(\"Drop in Adjusted R-squared with the removal of each variable\", fontsize=16)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Variance Inflation Factors\n",
    "# \n",
    "# Create the design matrix (X) using Patsy\n",
    "# Use the same formula as in the model, minus the response variable\n",
    "X = patsy.dmatrix(\n",
    "    \" + \".join([\n",
    "        \"tm_4\", \"tm_3\", \"tm_2\", \"tm_1\", \"t0\", \"t1\", \"t2\", \"t3\", \"t4\", \"t5\",\n",
    "        \"PC_AGI\", \"prev_2yr_avg_hpi_change\", \"prev_5yr_avg_hpi_change\", \"C(Year)\"\n",
    "    ]),\n",
    "    data=df_balanced_5,\n",
    "    return_type='dataframe'\n",
    ")\n",
    "\n",
    "# Calculate VIFs\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"feature\"] = X.columns\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Display top VIF values\n",
    "vif_df.sort_values(by=\"VIF\", ascending=False, inplace=True)\n",
    "print(vif_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not add ZIP FEs to the model? Our intuition says that we should leave them out. Given the shortness of the panel, \n",
    "# adding thousands of ZIP FEs will soak up all the inter-ZIP variance leaving precious little for the event coefficients.\n",
    "# It will also inflate the variance associated with the coefficient estimates making them less precise.\n",
    "# A better alternative to ZIP FEs is the PC_AGI covariate which we expect to capture much of the \"personality\" of a ZIP\n",
    "# especially in the way that it affects house price growth that the ZIP FE might have otherwise represented in a longer panel.  \n",
    "\n",
    "# At any rate, leaving out area FEs in a TWFE is an unconventional choice. So we need to be clear on why we want to do it.\n",
    "# \n",
    "# Let's try training the model by adding ZIP FEs to see what happens.\n",
    "\n",
    "formula = \"np.log(HPI) ~ \" + \" + \".join(event_time_dummies.columns) + \" + PC_AGI + prev_2yr_avg_hpi_change + prev_5yr_avg_hpi_change + C(Year) + C(Postcode)\"\n",
    "\n",
    "model = ols(formula, data=df_balanced_5)\n",
    "\n",
    "cluster_groups = df_balanced_5.loc[model.data.row_labels, 'Postcode']\n",
    "\n",
    "model_results = model.fit(cov_type='cluster', cov_kwds={'groups': cluster_groups})\n",
    "\n",
    "#model_results = model.fit(cov_type='HC1')\n",
    "\n",
    "# Display results\n",
    "print(model_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving out all covariates and considering only the year and the ZIP FEs and the event dummies will also produce the same sort of result.\n",
    "formula = \"np.log(HPI) ~ \" + \" + \".join(event_time_dummies.columns) + \" + C(Year) + C(Postcode)\"\n",
    "\n",
    "model = ols(formula, data=df_balanced_5)\n",
    "\n",
    "cluster_groups = df_balanced_5.loc[model.data.row_labels, 'Postcode']\n",
    "\n",
    "model_results = model.fit(cov_type='cluster', cov_kwds={'groups': cluster_groups})\n",
    "\n",
    "#model_results = model.fit(cov_type='HC1')\n",
    "\n",
    "# Display results\n",
    "print(model_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In both cases, notice the following about the above result:\n",
    "# R² explosion (0.288 --> 0.995):  As expected the ZIP FEs soak up all the variance leaving only the intra-ZIP over-time variance to be measured by the event time dummies.\n",
    "# But that's not what we want these dummies to measure!\n",
    "# \n",
    "# Event-time coefficients flip sign from positive to negative: Does this mean Starbucks preferentially seeks out neighborhoods with a nagative house price trend? That doesn't make sense. \n",
    "# At any rate, we shouldn't read too much into the flipped signs as most of the inter-ZIP variance is already captured by the ZIP FEs,\n",
    "#  and the event coefficients are probably measuring changes with respect to their respective mean price level instead of with respect to the controls. \n",
    "# \n",
    "# Very tiny eigenvalue / singularity warning, and rank deficiency: Notice the BIG RED FLAGS waved by Statsmodels in the Notes section and also warning about rank deficiency.\n",
    "# The almost zero eigenvalue implies that the X matrix (the regressors) are almost perfectly colinear. They are almost perfect linear combinations of other regressors.\n",
    "# The rank warning confirms this. Out of the 1000+ regressors we added, only 20 or so were found to be independent.\n",
    "# In such a circumstance, the X'X matrix that is at the heart of the OLS estimator ((X′X)^(−1)X′y) is basically non-invertible).\n",
    "# In plain words, this is a terrible model.\n",
    "# \n",
    "# The solution lies in the insight that a neighborhood (a ZIP) projects a certain \"personality\" that is multi-facted.\n",
    "# Different facets of this personality come in to play in determining (directly or indirectly) the attractiveness of neighborhood and thereby, the house price. \n",
    "# Important facets such as neighboorhood affluence, gentrification, and historical price momentum can be presumed to be strong determiners of house price growth.\n",
    "# Given the problems we saw with ZIP FEs, we instead included PC_AGI and the historical 2 year and 5 year price momentum variables as covariates.\n",
    "#  \n",
    "# We can run some (indirect) tests to validate the suitability of these covariates for capturing much of the price variance that ZIP FEs would have otherwise capatured:\n",
    "# \n",
    "# VIF analysis\n",
    "# \n",
    "# R² of covariates on ZIP FEs and Year FEs. How much of the variance of covariates is explained by the FEs?\n",
    "# If R^2 is very high when X = C(Postcode), then C(Postcode) explains much of the cross-sectional variance in the covariates. \n",
    "# It soaks up the same signal covariates carry, and adding it will inevitably inflate standard errors and destabilize event\n",
    "# coefficients in a short panel such as ours.\n",
    "# \n",
    "for var in [\"PC_AGI\", \"prev_2yr_avg_hpi_change\", \"prev_5yr_avg_hpi_change\"]:\n",
    "    m = smf.ols(f\"{var} ~ C(Postcode)\", data=df_balanced_5).fit()\n",
    "    print(var, \"R^2 =\", round(m.rsquared, 3))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Two-way demeaning then VIF on continuous covariates\n",
    "# 1. Two-way demean each continuous regressor (and, possibly, each event dummy) by ZIP and by Year:\n",
    "# 2. Build a matrix with the de-meaned continuous regressors only (e.g., PC_AGI, prev_2yr, prev_5yr).\n",
    "# 3. Compute standard VIFs on this residualized X to answer the question:\n",
    "#    How collinear are these covariates with each other after the FE information is absorbed?\n",
    "#    If they’re high, it confirms collinearity even in the within space; if they’re modest,\n",
    "#    it shows the main collinearity was between the covariates and the FE.\n",
    "\n",
    "def tw_demean(s, g_zip, g_year):\n",
    "    return s - s.groupby(g_zip).transform('mean') - s.groupby(g_year).transform('mean') + s.mean()\n",
    "\n",
    "Z = pd.DataFrame({\n",
    "    \"PC_AGI_dm\": tw_demean(df_balanced_5[\"PC_AGI\"], df_balanced_5[\"Postcode\"], df_balanced_5[\"Year\"]),\n",
    "    \"p2_dm\": tw_demean(df_balanced_5[\"prev_2yr_avg_hpi_change\"], df_balanced_5[\"Postcode\"], df_balanced_5[\"Year\"]),\n",
    "    \"p5_dm\": tw_demean(df_balanced_5[\"prev_5yr_avg_hpi_change\"], df_balanced_5[\"Postcode\"], df_balanced_5[\"Year\"]),\n",
    "})\n",
    "Z = Z.dropna()\n",
    "\n",
    "vifs = []\n",
    "for i, col in enumerate(Z.columns):\n",
    "    vifs.append((col, variance_inflation_factor(Z.values, i)))\n",
    "print(vifs)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Finally, also analyze Within vs between SD of covariates\n",
    "\n",
    "def within_sd(s, g):\n",
    "    return np.std(s - s.groupby(g).transform('mean'), ddof=1)\n",
    "def between_sd(s, g):\n",
    "    return np.std(s.groupby(g).transform('mean'), ddof=1)\n",
    "\n",
    "for var in [\"PC_AGI\", \"prev_2yr_avg_hpi_change\", \"prev_5yr_avg_hpi_change\"]:\n",
    "    w = within_sd(df_balanced_5[var], df_balanced_5[\"Postcode\"])\n",
    "    b = between_sd(df_balanced_5[var], df_balanced_5[\"Postcode\"])\n",
    "    print(var, \"within_sd=\", round(w,3), \"between_sd=\", round(b,3), \"within/between=\", round(w/b,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see two things very clearly from the above tests:\n",
    "# 1. The variance in PC_AGI (which we assume to be a proxy of neighborhood afluence and gentrification) is very strongly correlated with ZIP FEs,\n",
    "#  and an overwhelming fraction of it (as would be expected) is cross-sectional.\n",
    "# 2. Among the covariates, there is essentially no collinearity left after you partial out the fixed effect elements. \n",
    "# These results in our opinion, make this set of covariates (and especially PC_AGI) a strong substitute for ZIP FEs, especially given the serious structural issues we encountered after adding ZIP FEs to our panel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine cohort-level heterogeneity. To do so, we'll take inspiration from the Goodman-Bacon (2021) decomposition of the TWFE into cohort-wise 2x2 DiD models.\n",
    "# We'll plot both the cohort-wise ATT (Average Treatment Effect on Treated)(g) and the cohort-wise trend of the ATT(g) across event times τ = -4 through +5.\n",
    "\n",
    "# The following is simply the code that generates a balanced training panel that we saw earlier, except that this version builds it for a single cohort like for only g=2020\n",
    "def generate_balanced_panel(df_all_w_agi_by_year):\n",
    "\n",
    "    # Select relevant columns for score based matching (Uses MAHALANOBIS distance)\n",
    "    matching_features = ['PC_AGI', 'prev_2yr_avg_hpi_change', 'prev_5yr_avg_hpi_change']\n",
    "\n",
    "    # Drop all rows where the features to match have missing values\n",
    "    # df_matching will be used to draw the pool of treatment and matching control ZIPs\n",
    "    df_matching = df_all_w_agi_by_year.dropna(subset=matching_features)\n",
    "    #print('len(df_matching)=',len(df_matching))\n",
    "\n",
    "    # Fit a standard scaler to the matching_features vectors so that they can be standardized by\n",
    "    # removing the mean and scaling to unit variance.\n",
    "    scaler = StandardScaler()\n",
    "    # Note that df_matching[matching_features].values is an array of size (len(df_matching), len(matching_features)) i.e. (200597, 3)\n",
    "    scaler = scaler.fit(df_matching[matching_features].values)\n",
    "\n",
    "    # Extract the rows corresponding to ZIPs in which Starbucks opened in the Feb 2017 to Dec 2021 timeframe.\n",
    "    # Recall that this set of rows appear in df_matching as a result of the following sequence of Dataframe joins:\n",
    "    # df_zip_first_open_year with df_hpi_filtered to get df_all\n",
    "    # df_all with df_agi_all to get df_all_w_agi_by_year\n",
    "    df_treatment = df_matching[df_matching['starbucks_opened'] == 1].copy()\n",
    "    #print('len(df_treatment)=',len(df_treatment))\n",
    "\n",
    "    # Create a dataframe to store matched controls\n",
    "    df_matched_control = pd.DataFrame()\n",
    "\n",
    "    # Extract the set of unique treatment ZIPs from the set of treatment rows \n",
    "    treatment_zips = df_treatment['Postcode'].unique()\n",
    "    #print('len(treatment_zips)=',len(treatment_zips))\n",
    "\n",
    "    # Create a pool of candidate control ZIPs\n",
    "    df_control_pool = df_matching[df_matching['starbucks_opened'] == 0].copy()\n",
    "    #print('len(df_control_pool)=',len(df_control_pool))\n",
    "\n",
    "    count = 1\n",
    "    # Loop over each treatment ZIP and find the best match\n",
    "    for zip_code in treatment_zips:\n",
    "        # Get first_open_year of treatment ZIP\n",
    "        first_open_year = df_treatment[df_treatment['Postcode'] == zip_code]['first_open_year'].iloc[0]\n",
    "        match_year = first_open_year - 1  # Year before Starbucks opened\n",
    "\n",
    "        # Extract treatment ZIP profile for that match_year\n",
    "        treatment_profile = df_treatment[\n",
    "            (df_treatment['Postcode'] == zip_code) & (df_treatment['Year'] == match_year)\n",
    "            ][matching_features].values\n",
    "\n",
    "        if len(treatment_profile) == 0:\n",
    "            print(count,'/',len(treatment_zips),' ZIP=',zip_code,\n",
    "                ' first_open_year=',first_open_year, ' treatment_profile=',treatment_profile,\n",
    "                ' SKIPPING')\n",
    "            continue\n",
    "\n",
    "        standardized_treatment_profile = scaler.transform(treatment_profile)\n",
    "\n",
    "        # Compute distance between treatment ZIP and all control ZIPs\n",
    "\n",
    "        # Extract control ZIP profiles\n",
    "        control_profiles = df_control_pool[matching_features].values\n",
    "        if len(control_profiles) == 0:\n",
    "            print(count,'/',len(treatment_zips),' ZIP=',zip_code,\n",
    "                ' first_open_year=',first_open_year, ' treatment_profile=',treatment_profile,\n",
    "                'len(control_profiles)=',len(control_profiles), ' SKIPPING')\n",
    "            continue\n",
    "\n",
    "        standardized_control_profiles = scaler.transform(control_profiles)\n",
    "\n",
    "        cov_matrix = np.cov(standardized_control_profiles.T)\n",
    "\n",
    "        # Compute the inverse of the covariance matrix\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "        distances = cdist(standardized_treatment_profile, standardized_control_profiles,\n",
    "            metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "        # print(count,'/',len(treatment_zips),' ZIP=',zip_code,\n",
    "        #     ' first_open_year=',first_open_year, ' treatment_profile=',treatment_profile,\n",
    "        #     ' len(control_profiles)=',len(control_profiles),\n",
    "        #     ' len(distances)=',len(distances[0]))\n",
    "\n",
    "        # Find the best matching control ZIP\n",
    "        best_match_idx = np.argmin(distances)\n",
    "        best_match_zip = df_control_pool.iloc[best_match_idx]\n",
    "\n",
    "        df_best_match_zip_rows = df_control_pool[df_control_pool['Postcode']==best_match_zip['Postcode']]\n",
    "\n",
    "        # Append the best match to df_matched_control\n",
    "        df_matched_control = pd.concat([df_matched_control, df_best_match_zip_rows])\n",
    "\n",
    "        # Sort matched control ZIPs for consistency\n",
    "        df_matched_control = df_matched_control.sort_values(by=['Postcode', 'Year'])\n",
    "\n",
    "        # Remove the selected control ZIP from further consideration\n",
    "        df_control_pool = df_control_pool[df_control_pool['Postcode'] != best_match_zip['Postcode']]\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "    # Concatenate treatment and matched control ZIPs\n",
    "    df_balanced = pd.concat([df_treatment, df_matched_control], ignore_index=True)\n",
    "\n",
    "    df_balanced = df_balanced.astype({'Postcode': 'string', 'Year': 'int32', 'HPI': 'float64', 'starbucks_opened': 'int32', 'first_open_year': 'int32',\n",
    "        'event_time': 'int32', 'prev_5yr_avg_hpi_change': 'float64', 'prev_4yr_avg_hpi_change': 'float64',\n",
    "        'prev_3yr_avg_hpi_change': 'float64', 'prev_2yr_avg_hpi_change': 'float64'})\n",
    "    \n",
    "    #print(\"len(df_balanced)=\",len(df_balanced))\n",
    "    #print(\"len(df_matched_control)=\",len(df_matched_control))\n",
    "    #print(\"len(control_zips)=\",len(df_matched_control['Postcode'].unique()))\n",
    "    return df_balanced\n",
    "\n",
    "rows  = []\n",
    "cols = [\"g\", \"term\", \"coef\", \"std_err\", \"p_value\", \"ci_low\", \"ci_high\", \"pct_eff\", \"pct_ci_low\", \"pct_ci_high\", \"nobs\", \"df_model\", \"df_resid\"]\n",
    "\n",
    "df_all_w_agi[\"first_open_year\"].unique()\n",
    "for g in np.sort(a=df_all_w_agi[\"first_open_year\"].unique()):\n",
    "    print(\"g = \",g)\n",
    "    gg = int(g)\n",
    "    treatment_effect_ind = \"D_\" + str(gg)\n",
    "    df_all_w_agi_by_year = df_all_w_agi[(df_all_w_agi[\"first_open_year\"] == gg)]\n",
    "    df_balanced_5_by_year = generate_balanced_panel(df_all_w_agi_by_year)\n",
    "    df_treatment_by_year = df_balanced_5_by_year[df_balanced_5_by_year[\"starbucks_opened\"] == 1]\n",
    "    df_matched_control_by_year = df_balanced_5_by_year[df_balanced_5_by_year[\"starbucks_opened\"] == 0]\n",
    "\n",
    "    # Keep only event window (e.g., -5 to +5 years around event)\n",
    "    df_balanced_5_by_year = df_balanced_5_by_year[(df_balanced_5_by_year['event_time'] >= -5) & (df_balanced_5_by_year['event_time'] <= 5)]\n",
    "\n",
    "    # create D_it\n",
    "    df_balanced_5_by_year[treatment_effect_ind] = np.where(\n",
    "        (df_balanced_5_by_year[\"starbucks_opened\"] == 1) &\n",
    "        (df_balanced_5_by_year[\"first_open_year\"] == gg) & \n",
    "        (df_balanced_5_by_year[\"Year\"] >= gg),\n",
    "        1, \n",
    "        0\n",
    "    )\n",
    "\n",
    "    # Create event time dummies correctly\n",
    "    event_time_dummies = pd.get_dummies(df_balanced_5_by_year['event_time'], drop_first=True, prefix='t').astype(int)\n",
    "\n",
    "    # Rename columns: Replace negative signs and ensure valid variable names\n",
    "    event_time_dummies.columns = [col.replace('-', 'm_').replace('t_', 't') for col in event_time_dummies.columns]\n",
    "\n",
    "    # Merge with the main dataset\n",
    "    df_balanced_5_by_year = pd.concat([df_balanced_5_by_year, event_time_dummies], axis=1)\n",
    "\n",
    "    df_balanced_5_by_year['Postcode'] = df_balanced_5_by_year[\"Postcode\"].astype('object')\n",
    "\n",
    "    formula = \"np.log(HPI) ~ C(Year) + \" + treatment_effect_ind + \" + PC_AGI + prev_2yr_avg_hpi_change + prev_5yr_avg_hpi_change\"\n",
    "\n",
    "    model = ols(formula, data=df_balanced_5_by_year)\n",
    "\n",
    "    cluster_groups = df_balanced_5_by_year.loc[model.data.row_labels, 'Postcode']\n",
    "\n",
    "    model_results = model.fit(cov_type='cluster', cov_kwds={'groups': cluster_groups})\n",
    "\n",
    "    # Extract stats for the D_g term (may be dropped if collinear)\n",
    "    if treatment_effect_ind in model_results.params.index:\n",
    "        coef = model_results.params[treatment_effect_ind]\n",
    "        se = model_results.bse[treatment_effect_ind]\n",
    "        pval = model_results.pvalues[treatment_effect_ind]\n",
    "        ci_low, ci_high = model_results.conf_int(alpha=0.05).loc[treatment_effect_ind]\n",
    "    else:\n",
    "        coef = se = pval = ci_low = ci_high = np.nan\n",
    "\n",
    "    row = {\n",
    "        \"g\": g,\n",
    "        \"term\": treatment_effect_ind,\n",
    "        \"coef\": coef,\n",
    "        \"std_err\": se,\n",
    "        \"p_value\": pval,\n",
    "        \"ci_low\": ci_low,\n",
    "        \"ci_high\": ci_high,\n",
    "        \"nobs\": int(model_results.nobs),\n",
    "        \"df_model\": model_results.df_model,\n",
    "        \"df_resid\": model_results.df_resid,\n",
    "    }\n",
    "\n",
    "    # Transform log-point effects to percent: 100*(exp(beta)-1)\n",
    "    for k_in, k_out_lo, k_out_hi in [(\"coef\",\"pct_eff\",\"pct_ci_low\"), (\"ci_low\",\"pct_ci_low\",\"pct_ci_low\"), (\"ci_high\",\"pct_ci_high\",\"pct_ci_high\")]:\n",
    "        pass  # (placeholder to keep logic tidy)\n",
    "\n",
    "    row[\"pct_eff\"] = 100.0 * (np.exp(coef) - 1.0) if pd.notnull(coef) else np.nan\n",
    "    row[\"pct_ci_low\"] = 100.0 * (np.exp(ci_low) - 1.0) if pd.notnull(ci_low) else np.nan\n",
    "    row[\"pct_ci_high\"] = 100.0 * (np.exp(ci_high) - 1.0) if pd.notnull(ci_high) else np.nan\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "print(df_results)\n",
    "\n",
    "# Plot 𝜏_g with CIs (bar chart by entry year) to highlight the heterogeneity across cohorts.\n",
    "\n",
    "x = np.arange(len(df_results))\n",
    "y = df_results[\"pct_eff\"].values\n",
    "err_lower = y - df_results[\"pct_ci_low\"].values\n",
    "err_upper = df_results[\"pct_ci_high\"].values - y\n",
    "yerr = np.vstack([err_lower, err_upper])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\"o\", color=\"black\", capsize=5, elinewidth=1.5)\n",
    "plt.axhline(0, color=\"grey\", ls=\"--\", lw=1)\n",
    "plt.xticks(x, df_results[\"g\"].astype(int))\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(100))\n",
    "plt.ylabel(\"Estimated effect (%)\")\n",
    "plt.title(r\"Cohort-specific $\\tau_g$ with 95% CIs\")\n",
    "plt.tight_layout();\n",
    "plt.show()\n",
    "\n",
    "# Perform cohort-specific decomposition of the event time coefficients \n",
    "\n",
    "def parse_tau_from_col(col):\n",
    "    # 'tm_5' -> -5; 't0' -> 0; 't3' -> 3\n",
    "    if col.startswith(\"tm_\"):\n",
    "        return -int(col.split(\"tm_\")[1])\n",
    "    elif col.startswith(\"t\"):\n",
    "        return int(col.split(\"t\")[1])\n",
    "    raise ValueError(f\"Unrecognized event-dummy name: {col}\")\n",
    "\n",
    "rows_entry_year_cohorts = []\n",
    "cols_entry_year_cohorts = [\"g\", \"D_g\", \"tau_g\", \"std_err\", \"p_value\", \"ci_low\", \"ci_high\", \"pct_eff\", \"pct_ci_low\", \"pct_ci_high\", \"nobs\", \"df_model\", \"df_resid\"]\n",
    "\n",
    "rows_evt_time_cohorts  = []\n",
    "cols_evt_time_cohorts = [\"g\", \"event_time\", \"term\", \"beta\", \"std_err\", \"p_value\", \"ci_low\", \"ci_high\", \"pct_eff\", \"pct_ci_low\", \"pct_ci_high\", \"nobs\", \"df_model\", \"df_resid\"]\n",
    "\n",
    "df_all_w_agi[\"first_open_year\"].unique()\n",
    "for g in np.sort(a=df_all_w_agi[\"first_open_year\"].unique()):\n",
    "    print(\"g = \",g)\n",
    "    gg = int(g)\n",
    "    treatment_effect_ind = \"D_\" + str(gg)\n",
    "    df_all_w_agi_by_year = df_all_w_agi[(df_all_w_agi[\"first_open_year\"] == gg)]\n",
    "    df_balanced_5_by_year = generate_balanced_panel(df_all_w_agi_by_year)\n",
    "    df_treatment_by_year = df_balanced_5_by_year[df_balanced_5_by_year[\"starbucks_opened\"] == 1]\n",
    "    df_matched_control_by_year = df_balanced_5_by_year[df_balanced_5_by_year[\"starbucks_opened\"] == 0]\n",
    "\n",
    "    # Keep only event window (e.g., -5 to +5 years around event)\n",
    "    df_balanced_5_by_year = df_balanced_5_by_year[(df_balanced_5_by_year['event_time'] >= -5) & (df_balanced_5_by_year['event_time'] <= 5)]\n",
    "\n",
    "    # create D_it\n",
    "    df_balanced_5_by_year[treatment_effect_ind] = np.where(\n",
    "        (df_balanced_5_by_year[\"starbucks_opened\"] == 1) &\n",
    "        (df_balanced_5_by_year[\"first_open_year\"] == gg) & \n",
    "        (df_balanced_5_by_year[\"Year\"] >= gg),\n",
    "        1, \n",
    "        0\n",
    "    )\n",
    "\n",
    "    # Create event time dummies correctly\n",
    "    event_time_dummies = pd.get_dummies(df_balanced_5_by_year['event_time'], drop_first=True, prefix='t').astype(int)\n",
    "\n",
    "    # Rename columns: Replace negative signs and ensure valid variable names\n",
    "    event_time_dummies.columns = [col.replace('-', 'm_').replace('t_', 't') for col in event_time_dummies.columns]\n",
    "\n",
    "    for event_time_dummy_name in event_time_dummies.columns:\n",
    "        interaction_effect_ind = \"Int_\" + event_time_dummy_name\n",
    "        df_balanced_5_by_year[interaction_effect_ind] = df_balanced_5_by_year[\"starbucks_opened\"]*event_time_dummies[event_time_dummy_name]\n",
    "    \n",
    "    # DON'T Merge with the main dataset\n",
    "    # df_balanced_5_by_year = pd.concat([df_balanced_5_by_year, event_time_dummies], axis=1)\n",
    "\n",
    "    df_balanced_5_by_year['Postcode'] = df_balanced_5_by_year[\"Postcode\"].astype('object')\n",
    "\n",
    "    int_terms = \" + \".join([f\"Int_{c}\" for c in event_time_dummies.columns])\n",
    "\n",
    "    evt_time_cohort_formula = f\"np.log(HPI) ~ C(Year) + {int_terms}\" + \" + PC_AGI + prev_2yr_avg_hpi_change + prev_5yr_avg_hpi_change\"\n",
    "    print(evt_time_cohort_formula)\n",
    "\n",
    "    # Build the model\n",
    "    evt_time_cohort_model = ols(evt_time_cohort_formula, data=df_balanced_5_by_year)\n",
    "\n",
    "    # Align to the estimation sample (patsy may drop NA rows)\n",
    "    idx = evt_time_cohort_model.data.row_labels\n",
    "    zip_labels = df_balanced_5_by_year.loc[idx, \"Postcode\"]\n",
    "\n",
    "    # Factorize to contiguous integer codes 0..G-1\n",
    "    groups_codes, _ = pd.factorize(zip_labels, sort=False)\n",
    "    # If there could be missing labels (NaN), replace -1 with a new code\n",
    "    if (groups_codes < 0).any():\n",
    "        groups_codes = np.where(groups_codes < 0, groups_codes.max() + 1, groups_codes)\n",
    "\n",
    "    # Sanity checks\n",
    "    assert groups_codes.ndim == 1\n",
    "    assert groups_codes.shape[0] == len(evt_time_cohort_model.endog)\n",
    "\n",
    "    # Fit with clustered SE\n",
    "    evt_time_cohort_model_results = evt_time_cohort_model.fit(\n",
    "        cov_type=\"cluster\",\n",
    "        cov_kwds={\"groups\": groups_codes}\n",
    "    )\n",
    "\n",
    "    # Collect coefficients for each interaction, map back to event_time\n",
    "    ci = evt_time_cohort_model_results.conf_int()\n",
    "    for c in event_time_dummies.columns:\n",
    "        term = f\"Int_{c}\"\n",
    "        if term in evt_time_cohort_model_results.params.index:\n",
    "            beta = evt_time_cohort_model_results.params[term]\n",
    "            se = evt_time_cohort_model_results.bse[term]\n",
    "            p = evt_time_cohort_model_results.pvalues[term]\n",
    "            lo, hi = ci.loc[term]\n",
    "            tau = parse_tau_from_col(c)  # numeric event time\n",
    "\n",
    "            rows_evt_time_cohorts.append({\n",
    "                \"g\": g,\n",
    "                \"event_time\": tau,\n",
    "                \"term\": term,\n",
    "                \"beta\": beta,\n",
    "                \"std_err\": se,\n",
    "                \"p_value\": p,\n",
    "                \"ci_low\": lo,\n",
    "                \"ci_high\": hi,\n",
    "                \"pct_eff\": 100.0 * (np.exp(beta) - 1.0),\n",
    "                \"pct_ci_low\": 100.0 * (np.exp(lo) - 1.0),\n",
    "                \"pct_ci_high\": 100.0 * (np.exp(hi) - 1.0),\n",
    "                \"nobs\": int(evt_time_cohort_model_results.nobs),\n",
    "                \"df_model\": evt_time_cohort_model_results.df_model,\n",
    "                \"df_resid\": evt_time_cohort_model_results.df_resid,\n",
    "            })\n",
    "\n",
    "    entry_year_cohort_formula = \"np.log(HPI) ~ C(Year) + \" + treatment_effect_ind + \" + PC_AGI + prev_2yr_avg_hpi_change + prev_5yr_avg_hpi_change\"\n",
    "\n",
    "    model = ols(entry_year_cohort_formula, data=df_balanced_5_by_year)\n",
    "\n",
    "    cluster_groups = df_balanced_5_by_year.loc[model.data.row_labels, 'Postcode']\n",
    "\n",
    "    entry_year_cohort_model_results = model.fit(cov_type='cluster', cov_kwds={'groups': cluster_groups})\n",
    "\n",
    "    # Extract stats for the D_g term (may be dropped if collinear)\n",
    "    if treatment_effect_ind in entry_year_cohort_model_results.params.index:\n",
    "        coef = entry_year_cohort_model_results.params[treatment_effect_ind]\n",
    "        se = entry_year_cohort_model_results.bse[treatment_effect_ind]\n",
    "        pval = entry_year_cohort_model_results.pvalues[treatment_effect_ind]\n",
    "        ci_low, ci_high = entry_year_cohort_model_results.conf_int(alpha=0.05).loc[treatment_effect_ind]\n",
    "    else:\n",
    "        coef = se = pval = ci_low = ci_high = np.nan\n",
    "\n",
    "    row = {\n",
    "        \"g\": g,\n",
    "        \"D_g\": treatment_effect_ind,\n",
    "        \"tau_g\": coef,\n",
    "        \"std_err\": se,\n",
    "        \"p_value\": pval,\n",
    "        \"ci_low\": ci_low,\n",
    "        \"ci_high\": ci_high,\n",
    "        \"nobs\": int(entry_year_cohort_model_results.nobs),\n",
    "        \"df_model\": entry_year_cohort_model_results.df_model,\n",
    "        \"df_resid\": entry_year_cohort_model_results.df_resid,\n",
    "    }\n",
    "\n",
    "    row[\"pct_eff\"] = 100.0 * (np.exp(coef) - 1.0) if pd.notnull(coef) else np.nan\n",
    "    row[\"pct_ci_low\"] = 100.0 * (np.exp(ci_low) - 1.0) if pd.notnull(ci_low) else np.nan\n",
    "    row[\"pct_ci_high\"] = 100.0 * (np.exp(ci_high) - 1.0) if pd.notnull(ci_high) else np.nan\n",
    "\n",
    "    rows_entry_year_cohorts.append(row)\n",
    "\n",
    "df_results_evt_time_cohorts = pd.DataFrame(rows_evt_time_cohorts, columns=cols_evt_time_cohorts)\n",
    "df_results_entry_year_cohorts = pd.DataFrame(rows_entry_year_cohorts, columns=cols_entry_year_cohorts)\n",
    "\n",
    "print(df_results_evt_time_cohorts)\n",
    "print(df_results_entry_year_cohorts)\n",
    "\n",
    "# Faceted plot for 𝛽𝜏,𝑔 (τ on x, % effect on y), baseline τ = −5\n",
    "\n",
    "def facet_event_study(tidy_es, baseline_tau=-5, ncols=2, figsize_per_panel=(6,3.2)):\n",
    "    cohorts = sorted(tidy_es[\"g\"].unique())\n",
    "    n = len(cohorts)\n",
    "    nrows = math.ceil(n / ncols)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(figsize_per_panel[0]*ncols,\n",
    "                                                    figsize_per_panel[1]*nrows),\n",
    "                             sharex=True, sharey=True)\n",
    "    axes = np.array(axes).reshape(nrows, ncols)\n",
    "\n",
    "    # global x range\n",
    "    xticks = sorted(tidy_es[\"event_time\"].unique())\n",
    "    for k, g in enumerate(cohorts):\n",
    "        ax = axes[k // ncols, k % ncols]\n",
    "        d = tidy_es[tidy_es[\"g\"] == g].sort_values(\"event_time\")\n",
    "        x = d[\"event_time\"].to_numpy()\n",
    "        y = d[\"pct_eff\"].to_numpy()\n",
    "        yerr = np.vstack([y - d[\"pct_ci_low\"].to_numpy(),\n",
    "                          d[\"pct_ci_high\"].to_numpy() - y])\n",
    "\n",
    "        ax.errorbar(x, y, yerr=yerr, fmt=\"o-\", capsize=4, linewidth=1.2, markersize=4, color=\"black\")\n",
    "        ax.axhline(0, color=\"grey\", ls=\"--\", lw=1)\n",
    "\n",
    "        # mark the omitted baseline τ = -5 for reference\n",
    "        ax.axvline(baseline_tau, color=\"grey\", ls=\":\", lw=1)\n",
    "        ax.text(baseline_tau, ax.get_ylim()[1]*0.9, f\"Baseline τ={baseline_tau}\",\n",
    "                ha=\"center\", va=\"top\", fontsize=8, color=\"grey\")\n",
    "\n",
    "        ax.set_title(f\"Cohort {int(g)}\", fontsize=11)\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter(100))\n",
    "        ax.grid(alpha=0.15)\n",
    "\n",
    "    # remove any empty panels\n",
    "    for k in range(n, nrows*ncols):\n",
    "        fig.delaxes(axes[k // ncols, k % ncols])\n",
    "\n",
    "    # labels\n",
    "    fig.suptitle(\"Cohort-specific event-study estimates (95% CIs)\", y=0.98, fontsize=13)\n",
    "    for ax in axes[-1, :]:\n",
    "        if ax in axes:\n",
    "            ax.set_xlabel(\"Event time (years since entry)\")\n",
    "    for r in range(nrows):\n",
    "        axes[r,0].set_ylabel(\"Effect (% vs baseline)\")\n",
    "\n",
    "    plt.tight_layout(rect=[0,0,1,0.96])\n",
    "    plt.show()\n",
    "\n",
    "facet_event_study(df_results_evt_time_cohorts, baseline_tau=-5, ncols=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BJS (2021) robustness with FAST cluster bootstrap (no refits)\n",
    "# - Absorbs ZIP & Year FE via PanelOLS; falls back to Year-FE-only if needed.\n",
    "# - τ window: [-5,5]; years: 2012..YEAR_MAX (currently 2022, but change to a later date like 2027 if you get a fresh dataset from FHFA in the next few years)\n",
    "# - Outputs ATT(τ), CIs/p-values, joint pretrend test, and a percent-scale plot.\n",
    "\n",
    "YEAR_MAX = 2022\n",
    "TAU_MIN, TAU_MAX = -5, 5\n",
    "TAU_WINDOW = list(range(TAU_MIN, TAU_MAX + 1))\n",
    "N_BOOT =  499  # speed versus accuracy knob; use 0 to skip bootstrap entirely\n",
    "SEED = 123\n",
    "PLOT_UNIFORM_BAND = True\n",
    "\n",
    "def log_to_pct(x):\n",
    "    return 100.0 * (np.exp(x) - 1.0)\n",
    "\n",
    "def clean_panel(df, year_max, tau_min, tau_max):\n",
    "    df = df.copy()\n",
    "    df = df.query(\"2012 <= Year <= @year_max and @tau_min <= event_time <= @tau_max\").copy()\n",
    "    df[\"Postcode\"] = df[\"Postcode\"].astype(str)\n",
    "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "    df[\"logHPI\"] = np.log(df[\"HPI\"])\n",
    "    df[\"treated\"] = (df[\"starbucks_opened\"] == 1)\n",
    "    df[\"treated_period\"] = df[\"treated\"] & (df[\"Year\"] >= df[\"first_open_year\"])\n",
    "    df[\"tau\"] = np.where(df[\"treated\"], df[\"event_time\"], np.nan)\n",
    "    return df.set_index([\"Postcode\",\"Year\"]).sort_index()\n",
    "\n",
    "def fit_predict_counterfactual_panelols(df_panel):\n",
    "    from linearmodels.panel import PanelOLS\n",
    "    df_u = df_panel.loc[~df_panel[\"treated_period\"]].copy()\n",
    "    # 0 treated-period rows in the training sample\n",
    "    assert not df_u[\"treated_period\"].any()\n",
    "\n",
    "    # counts\n",
    "    print(\"Obs used to fit untreated model:\", len(df_u))\n",
    "    print(\"Obs used for ATT (treated rows):\", df_panel.loc[df_panel[\"treated\"]].shape[0])\n",
    "\n",
    "    df_u[\"__const\"] = 1.0\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # suppress MissingValueWarning noise\n",
    "        res = PanelOLS(df_u[\"logHPI\"], df_u[[\"__const\"]],\n",
    "                       entity_effects=True, time_effects=True,\n",
    "                       drop_absorbed=True).fit(cov_type=\"clustered\",\n",
    "                                               cluster_entity=True)\n",
    "    df_panel = df_panel.copy()\n",
    "    df_panel[\"__const\"] = 1.0\n",
    "    try:\n",
    "        pred = res.predict(exog=df_panel[[\"__const\"]])  # newer linearmodels\n",
    "    except TypeError:\n",
    "        pred = res.predict(df_panel[[\"__const\"]])       # older signature\n",
    "    df_panel[\"yhat_no_treat\"] = (pred.predictions if hasattr(pred,\"predictions\")\n",
    "                                 else pd.Series(np.asarray(pred).ravel(), index=df_panel.index))\n",
    "    return df_panel\n",
    "\n",
    "def fit_predict_counterfactual_yearfe(df_panel):\n",
    "    import statsmodels.formula.api as smf\n",
    "    df_sm = df_panel.reset_index().copy()\n",
    "    m = smf.ols(\"logHPI ~ C(Year)\", data=df_sm.loc[~df_sm[\"treated_period\"]]).fit()\n",
    "    df_sm[\"yhat_no_treat\"] = m.predict(df_sm)\n",
    "    return df_sm.set_index([\"Postcode\",\"Year\"])\n",
    "\n",
    "def fit_predict_counterfactual(df_raw, year_max=YEAR_MAX, tau_min=TAU_MIN, tau_max=TAU_MAX):\n",
    "    df_panel = clean_panel(df_raw, year_max, tau_min, tau_max)\n",
    "\n",
    "    # Try PanelOLS; only fall back if the package is missing\n",
    "    try:\n",
    "        from linearmodels.panel import PanelOLS  # noqa: F401\n",
    "        have_panelols = True\n",
    "    except ImportError:\n",
    "        have_panelols = False\n",
    "\n",
    "    if have_panelols:\n",
    "        df_hat = fit_predict_counterfactual_panelols(df_panel)  # <- uses ZIP+Year FE\n",
    "        used_panelols = True\n",
    "    else:\n",
    "        print(\"[WARN] Year-FE fallback (install 'linearmodels' for ZIP+Year FE).\")\n",
    "        df_hat = fit_predict_counterfactual_yearfe(df_panel)\n",
    "        used_panelols = False\n",
    "\n",
    "    att = df_hat.loc[df_hat[\"treated\"], [\"tau\",\"treated_period\",\"logHPI\",\"yhat_no_treat\"]].copy()\n",
    "    att = att.join(df_hat[[\"treated\"]], how=\"left\").reset_index()  # keep Postcode for clustering\n",
    "    att[\"att\"] = att[\"logHPI\"] - att[\"yhat_no_treat\"]\n",
    "    return att, used_panelols\n",
    "\n",
    "def summarize_att_tau(att, tau_window=TAU_WINDOW):\n",
    "    return (att.loc[att[\"tau\"].isin(tau_window)]\n",
    "               .groupby(\"tau\")[\"att\"].mean()\n",
    "               .reindex(tau_window))\n",
    "\n",
    "def overall_post_effect(att):\n",
    "    return att.loc[att[\"treated_period\"], \"att\"].mean()\n",
    "\n",
    "def overall_post_effect_donut(att, drop_taus={-1,0,1}):\n",
    "    mask = att[\"treated_period\"] & (~att[\"tau\"].isin(drop_taus))\n",
    "    return att.loc[mask, \"att\"].mean()\n",
    "\n",
    "# Performs a FAST cluster bootstrap (no refits)\n",
    "# Reuses fitted counterfactuals; we resample clusters (ZIPs) and recompute aggregated ATT(τ).\n",
    "def fast_cluster_bootstrap_from_predictions(att, B=N_BOOT, seed=SEED, tau_window=TAU_WINDOW):\n",
    "    if B <= 0:\n",
    "        return None\n",
    "    rng = np.random.default_rng(seed)\n",
    "    att_tau = att.loc[att[\"tau\"].isin(tau_window)].copy()\n",
    "\n",
    "    # Precompute per-cluster sums and counts at each τ\n",
    "    g = (att_tau.groupby([\"Postcode\",\"tau\"])[\"att\"]\n",
    "         .agg([\"sum\",\"count\"]).rename(columns={\"sum\":\"s\",\"count\":\"n\"}).reset_index())\n",
    "    # Pivot to matrices for speed: rows=clusters, cols=taus\n",
    "    clusters = g[\"Postcode\"].astype(str).unique()\n",
    "    taus = tau_window\n",
    "    s_mat = pd.DataFrame(0.0, index=clusters, columns=taus)\n",
    "    n_mat = pd.DataFrame(0.0, index=clusters, columns=taus)\n",
    "    for _, row in g.iterrows():\n",
    "        s_mat.at[str(row[\"Postcode\"]), int(row[\"tau\"])] = row[\"s\"]\n",
    "        n_mat.at[str(row[\"Postcode\"]), int(row[\"tau\"])] = row[\"n\"]\n",
    "\n",
    "    boot = np.empty((B, len(taus)), dtype=float)\n",
    "    for b in range(B):\n",
    "        draw = rng.choice(clusters, size=len(clusters), replace=True)\n",
    "        # multiplicity (how many times each cluster was picked)\n",
    "        # using value_counts is fine; or use np.add.at with integer ids\n",
    "        w = pd.Series(draw).value_counts()\n",
    "        # Weighted sums across drawn clusters\n",
    "        S = (s_mat.reindex(w.index).multiply(w, axis=0)).sum(axis=0).values\n",
    "        N = (n_mat.reindex(w.index).multiply(w, axis=0)).sum(axis=0).values\n",
    "        boot[b, :] = np.divide(S, N, out=np.zeros_like(S), where=(N>0))\n",
    "\n",
    "    return pd.DataFrame(boot, columns=taus)\n",
    "\n",
    "def att_table_with_uncertainty(att_tau, boot_df):\n",
    "    idx = att_tau.index\n",
    "    if boot_df is None:\n",
    "        return pd.DataFrame({\"ATT_log\":att_tau.values, \"SE\":np.nan,\n",
    "                             \"CI_lo\":np.nan, \"CI_hi\":np.nan, \"p\":np.nan}, index=idx)\n",
    "    se = boot_df.std(axis=0, ddof=1).reindex(idx)\n",
    "    z = att_tau / se\n",
    "    from scipy.stats import norm\n",
    "    p = 2*(1 - norm.cdf(np.abs(z)))\n",
    "    ci_lo = att_tau - 1.96*se\n",
    "    ci_hi = att_tau + 1.96*se\n",
    "    out = pd.DataFrame({\"ATT_log\":att_tau, \"SE\":se, \"CI_lo\":ci_lo, \"CI_hi\":ci_hi, \"p\":p})\n",
    "    out[\"ATT_pct\"] = log_to_pct(out[\"ATT_log\"])\n",
    "    out[\"CI_lo_pct\"] = log_to_pct(out[\"CI_lo\"])\n",
    "    out[\"CI_hi_pct\"] = log_to_pct(out[\"CI_hi\"])\n",
    "    return out\n",
    "\n",
    "def uniform_band(att_tau, boot_df):\n",
    "    if boot_df is None:\n",
    "        return None, None\n",
    "    idx = att_tau.index\n",
    "    se = boot_df.std(axis=0, ddof=1).reindex(idx).replace(0.0, np.nan)\n",
    "    t_boot = (boot_df.subtract(att_tau, axis=1)).divide(se, axis=1)\n",
    "    c95 = t_boot.abs().max(axis=1).quantile(0.95)\n",
    "    uni_lo = att_tau - c95*se\n",
    "    uni_hi = att_tau + c95*se\n",
    "    return uni_lo, uni_hi\n",
    "\n",
    "def joint_pretrend_test(att_tau, boot_df, pre_taus=(-4,-3,-2,-1)):\n",
    "    from numpy.linalg import pinv\n",
    "    from scipy.stats import chi2\n",
    "    theta = att_tau.loc[list(pre_taus)].to_numpy()\n",
    "    if boot_df is None or np.any(~np.isfinite(theta)):\n",
    "        return {\"W\": np.nan, \"df\": len(pre_taus), \"p_asym\": np.nan, \"p_boot\": np.nan}\n",
    "    boot_pre = boot_df.loc[:, list(pre_taus)].to_numpy()\n",
    "    boot_centered = boot_pre - theta\n",
    "    Sigma = np.cov(boot_centered, rowvar=False)\n",
    "    W = float(theta.T @ pinv(Sigma) @ theta)\n",
    "    p_asym = 1 - chi2.cdf(W, df=len(pre_taus))\n",
    "    Qb = np.einsum('bi,ij,bj->b', boot_centered, pinv(Sigma), boot_centered)\n",
    "    p_boot = float(np.mean(Qb >= W))\n",
    "    return {\"W\": W, \"df\": len(pre_taus), \"p_asym\": p_asym, \"p_boot\": p_boot}\n",
    "\n",
    "def bjs_pre_donut_tests(att_tau, boot_df, drop_sizes=(1,2,3)):\n",
    "    pre_full = [-5,-4,-3,-2,-1]\n",
    "    rows = []\n",
    "    for k in drop_sizes:\n",
    "        drop = set(range(-k,0))                 # {−1,..,−k}\n",
    "        keep = [t for t in pre_full if t not in drop]\n",
    "        theta = att_tau.loc[keep].to_numpy()\n",
    "        mean_log = float(theta.mean()); mean_pct = log_to_pct(mean_log)\n",
    "\n",
    "        if boot_df is not None:\n",
    "            boot_keep = boot_df.loc[:, keep].to_numpy()\n",
    "            boot_centered = boot_keep - theta\n",
    "            Sigma = np.cov(boot_centered, rowvar=False)\n",
    "            W = float(theta.T @ pinv(Sigma) @ theta)\n",
    "            Qb = np.einsum('bi,ij,bj->b', boot_centered, pinv(Sigma), boot_centered)\n",
    "            p_boot = float(np.mean(Qb >= W))\n",
    "            taus = np.asarray(keep, dtype=float)\n",
    "            X = np.column_stack([np.ones_like(taus), taus])\n",
    "            beta = lstsq(X, theta, rcond=None)[0]; slope_log = float(beta[1])\n",
    "            slopes = np.array([lstsq(X, boot_keep[b,:], rcond=None)[0][1] for b in range(boot_keep.shape[0])])\n",
    "            se_slope = slopes.std(ddof=1)\n",
    "            z = slope_log / se_slope; p_slope = 2*(1 - norm.cdf(abs(z)))\n",
    "            ci_lo_log = slope_log - 1.96*se_slope; ci_hi_log = slope_log + 1.96*se_slope\n",
    "        else:\n",
    "            W = p_boot = np.nan\n",
    "            taus = np.asarray(keep, dtype=float)\n",
    "            X = np.column_stack([np.ones_like(taus), taus])\n",
    "            slope_log = float(lstsq(X, theta, rcond=None)[0][1])\n",
    "            p_slope = ci_lo_log = ci_hi_log = np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"k\": k, \"keep\": keep,\n",
    "            \"mean_pre_%\": round(mean_pct, 2),\n",
    "            \"Wald_p\": p_boot,\n",
    "            \"slope_%/τ\": round(log_to_pct(slope_log), 2),\n",
    "            \"slope_CI_lo_%\": None if np.isnan(ci_lo_log) else round(log_to_pct(ci_lo_log), 2),\n",
    "            \"slope_CI_hi_%\": None if np.isnan(ci_hi_log) else round(log_to_pct(ci_hi_log), 2),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def make_presentation_figure(att_tau, boot_df, overall_post_log, overall_post_donut_log,\n",
    "                             pre_drop_sizes=(1,2,3), show_uniform=True):\n",
    "    # Uncertainty + uniform band\n",
    "    att_tab = att_table_with_uncertainty(att_tau, boot_df)\n",
    "    uni = uniform_band(att_tau, boot_df) if show_uniform else (None, None)\n",
    "\n",
    "    # Pre-trend fit on τ ∈ [-5,-1]\n",
    "    pre_idx = [-5,-4,-3,-2,-1]\n",
    "    slope, intercept = np.polyfit(pre_idx, att_tau.loc[pre_idx].values, 1)\n",
    "    taus = att_tau.index.values\n",
    "    baseline = intercept + slope*taus   # continuation of pre-trend\n",
    "\n",
    "    # Pre-only donut summary table\n",
    "    pre_tbl = bjs_pre_donut_tests(att_tau, boot_df, drop_sizes=pre_drop_sizes)\n",
    "\n",
    "    # Figure layout\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "    fig = plt.figure(figsize=(10, 6.4))\n",
    "    gs = GridSpec(3, 1, height_ratios=[3.0, 0.25, 1.25], hspace=0.25)\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    # Main plot: ATT% with CI + uniform band + pre-trend continuation\n",
    "    x = att_tab.index.values\n",
    "    ax.fill_between(x, att_tab[\"CI_lo_pct\"], att_tab[\"CI_hi_pct\"], alpha=0.20, label=\"95% CI\")\n",
    "    if uni[0] is not None:\n",
    "        ax.fill_between(x, log_to_pct(uni[0]), log_to_pct(uni[1]), alpha=0.12, label=\"Uniform 95% band\")\n",
    "    ax.plot(x, att_tab[\"ATT_pct\"], marker=\"o\", linewidth=1.8, label=\"ATT(τ) %\")\n",
    "    ax.plot(x, log_to_pct(pd.Series(baseline, index=att_tab.index)), linestyle=\"--\", linewidth=1.2, label=\"Pre-trend continuation\")\n",
    "    ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.7)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xlabel(\"Event time (τ)\")\n",
    "    ax.set_ylabel(\"Effect on HPI (%)\")\n",
    "    ax.set_title(f\"Dynamic ATT by event time (BJS)\\nOverall post ≈ {log_to_pct(overall_post_log):.1f}% (donut: {log_to_pct(overall_post_donut_log):.1f}%)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "    # Caption row\n",
    "    ax_cap = fig.add_subplot(gs[1, 0]); ax_cap.axis('off')\n",
    "    pre_wald = joint_pretrend_test(att_tau, boot_df)\n",
    "    cap = (f\"Notes: Pointwise 95% CIs from cluster bootstrap (B={0 if boot_df is None else boot_df.shape[0]}). \"\n",
    "           f\"Estimated pre-trend slope ≈ {log_to_pct(slope):.2f}% per τ.\")\n",
    "    ax_cap.text(0, 0, cap, va=\"center\")\n",
    "\n",
    "    # Table row (pre-only donuts)\n",
    "    ax_tbl = fig.add_subplot(gs[2, 0]); ax_tbl.axis('off')\n",
    "    show_cols = [\"k\",\"keep\",\"mean_pre_%\",\"Wald_p\",\"slope_%/τ\",\"slope_CI_lo_%\",\"slope_CI_hi_%\"]\n",
    "    table = ax_tbl.table(cellText=pre_tbl[show_cols].values,\n",
    "                         colLabels=[\"drop k\",\"kept τ\",\"mean pre (%)\",\"Wald p\",\"slope (%/τ)\",\"slope CI lo\",\"slope CI hi\"],\n",
    "                         loc=\"center\")\n",
    "    table.auto_set_font_size(False); table.set_fontsize(9); table.scale(1,1.2)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Run everything\n",
    "df_raw = df_all_w_agi.copy()\n",
    "\n",
    "# Fit the model\n",
    "att_cells, used_panelols = fit_predict_counterfactual(df_raw, YEAR_MAX, TAU_MIN, TAU_MAX)\n",
    "att_tau = summarize_att_tau(att_cells, TAU_WINDOW)\n",
    "overall_post = overall_post_effect(att_cells)\n",
    "overall_post_donut = overall_post_effect_donut(att_cells, {-1,0,1})\n",
    "\n",
    "print(f\"Estimator: {'PanelOLS (ZIP+Year FE absorbed)' if used_panelols else 'Year-FE only (fallback)'}\")\n",
    "print(\"ATT by event time (log points):\")\n",
    "print(att_tau)\n",
    "print(f\"\\nOverall post ATT (log): {overall_post:.4f}  ->  %: {log_to_pct(overall_post):.2f}%\")\n",
    "print(f\"Overall post ATT, donut τ∉{{-1,0,1}} (log): {overall_post_donut:.4f}  ->  %: {log_to_pct(overall_post_donut):.2f}%\")\n",
    "\n",
    "# Perform a FAST bootstrap (no refits)\n",
    "boot_df = fast_cluster_bootstrap_from_predictions(att_cells, B=N_BOOT, seed=SEED, tau_window=TAU_WINDOW)\n",
    "\n",
    "# Tests\n",
    "att_tab = att_table_with_uncertainty(att_tau, boot_df)\n",
    "wald = joint_pretrend_test(att_tau, boot_df, pre_taus=(-4,-3,-2,-1))\n",
    "print(\"\\nJoint pre-trend test on leads τ ∈ {-4,-3,-2,-1}:\")\n",
    "print(wald)\n",
    "\n",
    "# Build uniform bands\n",
    "uni = uniform_band(att_tau, boot_df) if PLOT_UNIFORM_BAND else (None, None)\n",
    "\n",
    "# Build a table with rates in percentage terms\n",
    "display_cols = [\"ATT_log\",\"ATT_pct\",\"SE\",\"CI_lo_pct\",\"CI_hi_pct\",\"p\"]\n",
    "pretty = att_tab[display_cols].copy()\n",
    "pretty[\"ATT_pct\"] = pretty[\"ATT_pct\"].round(2)\n",
    "pretty[\"SE\"] = pretty[\"SE\"].round(4)\n",
    "pretty[\"CI_lo_pct\"] = pretty[\"CI_lo_pct\"].round(2)\n",
    "pretty[\"CI_hi_pct\"] = pretty[\"CI_hi_pct\"].round(2)\n",
    "print(\"\\nATT(τ) with pointwise 95% CIs and p-values (percent scale):\")\n",
    "print(pretty)\n",
    "\n",
    "# Plot everything\n",
    "fig = make_presentation_figure(att_tau, boot_df, overall_post, overall_post_donut,\n",
    "                               pre_drop_sizes=(1,2,3), show_uniform=True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
